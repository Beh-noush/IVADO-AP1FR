{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "B9ALu78bhPXM"
   },
   "source": [
    "# ÉCOLE IVADO/Mila EN APPRENTISSAGE PROFOND\n",
    "# SESSION D'AUTOMNE 2018 \n",
    "# Tutoriel : Réseaux de neurones récurrents (RNN)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MIH_PfZV1rNa"
   },
   "source": [
    "## Auteurs\n",
    "\n",
    "Francis Grégoire <francis.gregoire@rd.mila.quebec>\n",
    "\n",
    "Jeremy Pinto <jeremy.pinto@rd.mila.quebec>\n",
    "\n",
    "Jean-Philippe Reid <Jean-Philippe.Reid@ElementAI.com>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OYrPFO1p1rX1"
   },
   "source": [
    "## Préface\n",
    "\n",
    "Ce tutoriel contient deux exemples ayant comme objectif d'introduire des concepts fondamentaux sur les réseaux neuronaux récurrents (RNN et LSTM).\n",
    "\n",
    "La première tâche consiste d'un exemple plutôt élémentaire servant à mettre en évidence l'avantage d'utiliser un modèle LSTM contre un simple RNN.\n",
    "\n",
    "Dans la deuxième tâche, nous allons mettre en valeur un modèle LSTM de manière plus concrète en développant un modèle de langue neuronal pour générer du nouveau texte. Dans cet exemple, vous allez apprendre comment faire le prétraitement des données textuelles dans le but de réussir à entraîner un modèle de langue neuronal de manière efficace."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ArGnixElhPXN"
   },
   "source": [
    "---\n",
    "# Initialisation et importation des librairies\n",
    "\n",
    "Pour assurer le bon fonctionnement de ce notebook sur Colab, il est nécessaire d'installer quelques librairies à l'aide de `pip`. Tout d'abord, assurez-vous d'être connectés au notebook (✓ CONNECTED en haut à droite). Exécutez ensuite la cellule suivante en la sélectionnant et en cliquant `shift`+`Enter`. L'installation peut prendre quelques minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "028-EhOGhPXO"
   },
   "outputs": [],
   "source": [
    "!pip3 install torch torchvision matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UwebZdYMhPXT"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import time\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "use_gpu = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
    "\n",
    "# Seed pour reproduire les résultats.\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "\n",
    "print(\"PyTorch version: \", torch.__version__)\n",
    "print(\"GPU available: {}\".format(use_gpu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7SLcPxj5z2vX"
   },
   "source": [
    "---\n",
    "# Tâche 1: Sommes de nombres"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4Y-PnqdMhPXX"
   },
   "source": [
    "\n",
    "## Objectif\n",
    "\n",
    "L'objectif de cet exemple est de construire un modèle capable de faire la somme d'une série de nombres. Il s'agit d'une tâche très simple (une calculatrice de poche peut facilement l'exécuter!) et servira à souligner certaines limitations des RNN. L'ensemble de données est facile à générer et nous permet de rapidement tester la capacité à modéliser une longue séquence entre un RNN traditionnel et un LSTM.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nihkB--rz6WL"
   },
   "source": [
    "## Ensemble de données\n",
    "\n",
    "L'ensemble de données est constitué d'un ensemble de séquences de nombres de longueur $seq\\_len$ où une cible est associée à chacune. Dans notre cas, la cible d'une séquence est associée à la somme des nombres de la séquence. Ainsi, pour un exemple donné nous avons comme variable d'entrée un vecteur $\\mathbf x^{(i)} = \\left[x_{1}^{i}, x_{2}^{i}, \\dots, x_{T}^{N}\\right]$ de longueur $seq\\_len=T$ et comme cible la variable $y^{(i)}$ donnée par:\n",
    "\n",
    "\\begin{align}  \n",
    "y^{(i)}=\\sum_{j=1}^{seq\\_len}x^{(i)}_j.\n",
    "\\end{align}\n",
    "\n",
    "Où $j$ est l'indice de temps.\n",
    "\n",
    "Par exemple, pour un $\\mathbf x^{(i)}$ explicitement défini avec $seq\\_len=4$, nous avons:\n",
    "\n",
    "\n",
    "\\begin{align}  \n",
    "\\mathbf x^{(i)} &= \\left[ 4,-1,15,24\\right], \\, \\mathbf x^{(i)} \\in \\mathbb R^{4}; \\\\ \n",
    "y^{(i)} &= 42, \\, \\mathrm y^{(i)} \\in \\mathbb R.\n",
    "\\end{align}\n",
    "\n",
    "Nous allons nous servir de cet ensemble de données pour entrainer un RNN et un LSTM. Comme la cible est un nombre réel, nous devons utiliser une couche linéaire pour projeter le dernier état récurrent du RNN/LSTM, $h^{(i)}_{T}$, tel qu'illustré à la figure suivante:\n",
    "\n",
    "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.002.jpeg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zZNrVvS8hPXY"
   },
   "source": [
    "### Génération d'un ensemble de données\n",
    "\n",
    "Afin de mieux énoncer la tâche, nous définissons une fonction utilitaire qui nous permet de construire un ensemble de données aléatoires de `n_samples` séquences de longueur `seq_len` en utilisant la fonction [torch.randint()](https://pytorch.org/docs/stable/torch.html#torch.randint).La fonction `generate_data` prend en entrée les arguments suivants :\n",
    "- **n_samples** (int) : nombre de séquences à générer.\n",
    "- **seq_len** (int) : longueur de chaque séquence.\n",
    "- **input_dim** (int, optionnel) : dimension des données d'entrée. Valeur par défaut : 1.\n",
    "- **xmin** (float, optionnel) : valeur minimale possible dans la séquence. Valeur par défaut : -100.\n",
    "- **xmax** (float, optionnel) : valeur maximale possible dans la séquence. Valeur par défaut : -100.\n",
    "\n",
    "Elle renvoie un tuple de deux éléments correspondant respectivement à :\n",
    "- **X** ([torch.FloatTensor](https://pytorch.org/docs/stable/tensors.html)) : Un tenseur de forme $n\\_samples \\times seq\\_len \\times input\\_dim$ représentant un ensemble de séquences `n_samples`, chacune de longueur `seq_len`. Les éléments des séquences sont de dimension `input_dim`, c'est-à-dire qu'ils appartiennent à ${\\mathbb R}^{input\\_dim}$.\n",
    "- **Y** ([torche.FloatTensor](https://pytorch.org/docs/stable/tensors.html)) : Un tenseur de forme $n\\_samples \\times input\\_dim$ représentant la somme correspondante des éléments pour chaque séquence dans `X`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "nKFfyWFahPXZ"
   },
   "outputs": [],
   "source": [
    "def generate_data(n_samples, seq_len, input_dim, xmin=-100, xmax=100):\n",
    "    \"\"\"Générer des tenseurs X et Y dans l'intervalle [xmin, xmax].\n",
    "    \n",
    "    Args : \n",
    "      n_samples: int, nombre de séquences à générer.\n",
    "      seq_len: int, longueur de chaque séquence.\n",
    "      xmin: valeur minimale que peut prendre n'importe quel nombre d'une séquence.\n",
    "      xmax: valeur maximale que peut prendre n'imoprte quel nombre d'une séquence.\n",
    "    \n",
    "    Returns: retourne les séquences de nombres X et les cibles Y en format \n",
    "             torch.Tensor où X.shape = (n_samples, seq_len, 1) et\n",
    "             Y.shape = (n_samples, 1).\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # À Compléter\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-uhue_rShPXc"
   },
   "source": [
    "Le code suivant montre un exemple d'utilisation de la fonction `generate_data`. Dans cet exemple, nous générons 1000 séquences de nombres $\\in \\mathbb R$ (c'est-à-dire que `input_dim` est fixé à 1), chacune de longueur 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "nKFfyWFahPXZ"
   },
   "outputs": [],
   "source": [
    "n_samples = 1000\n",
    "seq_len = 4\n",
    "input_dim = 1\n",
    "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
    "print(\"Dimensions du tenseur X = {}\".format(X.shape))\n",
    "print(\"où n_samples = {}, seq_len = {}, input_dim = {}\".format(*X.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s1wHZ_PFhPXf"
   },
   "source": [
    "### Standardisation des données\n",
    "\n",
    "Afin d'aider notre modèle lors de l'entraînement, nous standardisons nos données de sorte qu'elles aient une moyenne de 0 et un écart type de 1. Afin de pouvoir retransformer nos données, nous devons conserver les valeurs de la moyenne et de l'écart type employées. \n",
    "\n",
    "Note: étant donné que nous échantillonnons à partir d'un ensemble uniformément distribué, l'écart type devrait approcher $\\frac{(xmax-xmin)}{\\sqrt{12}}$ et la moyenne $\\frac{(xmax-xmin)}{2}$.\n",
    "\n",
    "La fonction suivante effectue une telle opération de normalisation. Elle prend en entrée un tenseur **X** (de forme $n\\_samples \\times seq\\_len \\times input\\_dim$) que nous souhaitons normaliser et retourne un tuple de 4 éléments correspondant respectivement à :\n",
    "- **Xs** : la version standardisée de X, de forme $n\\_samples \\times seq\\_len \\times input\\_dim$\n",
    "- **Ys** : la nouvelle somme des séquences de Xs, de forme $n\\_samples \\times input\\_dim$\n",
    "- **moyenne** : la moyenne de X, flotter.\n",
    "- **stdev** : l'écart type de X, flotteur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UdL_y85mhPXg"
   },
   "outputs": [],
   "source": [
    "def standardize(X):\n",
    "    \"\"\"Cette fonction standardise le tenseur X. \n",
    "    Args:\n",
    "      X: torch.Tensor.\n",
    "    \n",
    "    Returns:\n",
    "      X: torch.Tensor standardisé.\n",
    "      Y: torch.Tensor, somme du tenseur X.\n",
    "      mean: torch.mean, moyenne du tenseur X\n",
    "      std: torch.std, écart type de X\n",
    "    \"\"\"\n",
    "    \n",
    "    # À Compléter\n",
    "\n",
    "    return X, Y, mean, std\n",
    "\n",
    "\n",
    "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
    "X, Y, mean, std = standardize(X)\n",
    "print(\"moyenne = {:.4f}, écart-type = {:.4f}\".format(mean, std))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wCm8xalthPXj"
   },
   "source": [
    "## Implémentation d'un RNN\n",
    "\n",
    "Nous allons définir notre RNN en utilisant la classe [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn). Pour plus de détails sur l'implémentation de cette classe, vous pouvez consulter ce [tutoriel](https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#example-2-recurrent-net). Une fois initialisée, cette classe accepte comme données d'entrées `X` de dimensions `(seq_len, batch_size, input_size)` (avec `input_size=1` dans notre exemple). Nous devons ajouter une couche linéaire pour transformer la prédiction en sortie du RNN dans la même dimension que notre cible `Y` de dimensions `(batch_size, output_size)` (avec `output_size = input_size = 1` dans notre exemple). \n",
    "\n",
    "Pour définir l'architecture de notre RNN, nous utiliserons le module [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn) suivi d'une couche linéaire [torch.nn.Linear()](https://pytorch.org/docs/stable/nn.html#linear). Les méthodes suivantes sont à compléter:\n",
    "<ul>\n",
    "<li>La méthode `__init__()` qui définit les couches du modèle. </li>\n",
    "<li>La méthode `forward()` qui utilise les couches et des variables en entrée pour retourner une sortie (équivalent à une *forward pass*).</li>\n",
    "</ul>\n",
    "**Notes importantes**: \n",
    "\n",
    "* il peut être déroutant d'obtenir le dernier état récurrent de la dernière couche cachée d'un RNN, $h_{T}^{N}$, où $T$ est le dernier pas de temps et $N$ est la dernière couche cachée. Dans le cas d'un RNN, le dernier état récurrent de la dernière couche cachée peut être obtenu en indexant ces deux tenseurs de cette manière: `output[-1, :, :]` ou `hidden_T[-1]`.\n",
    "\n",
    "* Vous devez vous assurer que les dimensions de `X` en entrée soient conséquents avec l'implémentation de [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn). La fonction [tensor.transpose()](https://pytorch.org/docs/stable/tensors.html?highlight=transpose#torch.Tensor.transpose) peut vous être utile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice :\n",
    "\n",
    "Complétez ce morceau de code pour implémenter le réseau décrit ci-dessus en utilisant la classe [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn). Les arguments d'entrée sont les suivants :\n",
    "- **input_dim** : la dimension des données d'entrée\n",
    "- **output_dim** : la dimension des données de sortie\n",
    "- **hiden_size** : la taille de l'état caché du RNN\n",
    "- **n_couches** : le nombre de couches du RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3QCVSZGzhPXk"
   },
   "outputs": [],
   "source": [
    "class RNNLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
    "        super(RNNLinear, self).__init__()\n",
    "        \n",
    "        # À Compléter\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # À Compléter\n",
    "        # rappel: l'input d'un RNN doit être de dimensions (seq_len, batch_size, input_size)\n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple de la façon dont cette classe peut être utilisée pour prédire les valeurs des séquences en X."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3QCVSZGzhPXk"
   },
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "seq_len = 4\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "hidden_size = 20\n",
    "\n",
    "# Génération des données\n",
    "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
    "Xs, Ys, mean, std = standardize(X)\n",
    "\n",
    "# Déclarer le modèle RNN.\n",
    "model_rnn = RNNLinear(input_dim, output_dim, hidden_size, n_layers).to(device)\n",
    "\n",
    "# Transférer le modèle au bon matériel\n",
    "model_rnn = model_rnn.to(device)\n",
    "\n",
    "# Sauvegarder les poids initiaux du modèle.\n",
    "init_rnn_weights = copy.deepcopy(model_rnn.state_dict())\n",
    "\n",
    "# Transférer les données au bon matériel\n",
    "Xs = Xs.to(device)\n",
    "\n",
    "# Utiliser le RNN pour faire la prédiction de tous les exemples sans entraînement (c.à.d. sur les n_samples)\n",
    "# Vérifiez que les inputs d'entrée et sorties sont justes\n",
    "y_pred = model_rnn(X)\n",
    "print(\"Dimensions initiales des données en entrée: {}\".format(X.shape)) # (seq_len, batch_size, input_size)\n",
    "print(\"Dimensions des prédictions: {}\".format(y_pred.shape)) # (batch_size, input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jg8vsRduhPYH"
   },
   "source": [
    "## Implémentation d'un LSTM\n",
    "\n",
    "Nous allons maintenant implémenter un LSTM. Nous utilisons la classe [torch.nn.LSTM()](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM). Comme avec le RNN, nous devons ajouter une couche linéaire pour transformer la prédiction en sortie du LSTM dans la même dimension que notre cible `Y` de dimensions `(batch_size, output_size)` (avec `output_size = input_size = 1` dans notre exemple). Pour plus de détails sur l'implémentation de cette classe, consultez ce [tutoriel](https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#example-2-recurrent-net).\n",
    "\n",
    "Pour définir l'architecture de notre LSTM, les méthodes suivantes sont à compléter:\n",
    "<ul>\n",
    "<li>La méthode `__init__()` qui définit les couches du modèle. </li>\n",
    "<li>La méthode `forward()` qui utilise les couches et des variables en entrée pour retourner une sortie (équivalent à une forward pass).</li>\n",
    "</ul>\n",
    "\n",
    "**Notes importantes**: \n",
    "\n",
    "*  il peut être déroutant d'obtenir le dernier état récurrent de la dernière couche cachée d'un LSTM, $h_{T}^{N}$, où $T$ est le dernier pas de temps et $N$ est la dernière couche cachée. Dans le cas d'un LSTM, le dernier état récurrent de la dernière couche cachée peut être obtenu en indexant ces tenseurs de cette manière: `output[-1, :, :]` ou `hidden_T[0][-1]`.\n",
    "\n",
    "* Vous devez vous assurer que les dimensions de `X` en entrée soient conséquents avec l'implémentation de [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice :\n",
    "\n",
    "Complétez ce morceau de code pour implémenter le réseau décrit ci-dessus en utilisant la classe [torch.nn.LSTM()](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM). Les arguments d'entrée sont les suivants :\n",
    "- **input_dim** : la dimension des données d'entrée\n",
    "- **output_dim** : la dimension des données de sortie\n",
    "- **hiden_size** : la taille de l'état caché du RNN\n",
    "- **n_couches** : le nombre de couches du RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqMaUDDVhPYH"
   },
   "outputs": [],
   "source": [
    "class LSTMLinear(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size, hidden_size, n_layers):\n",
    "        \n",
    "        # À Compléter\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # À Compléter\n",
    "        # rappel: l'input d'un RNN doit être de dimensions (seq_len, batch_size, input_size)\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is an example of how this class can be used to predict the values of the sequences in Xs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lqMaUDDVhPYH"
   },
   "outputs": [],
   "source": [
    "n_samples = 50\n",
    "seq_len = 4\n",
    "input_dim = 1\n",
    "output_dim = 1\n",
    "n_layers = 2\n",
    "hidden_size = 20\n",
    "\n",
    "# Génération des données\n",
    "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
    "Xs, Ys, mean, std = standardize(X)\n",
    "\n",
    "# Déclarer le modèle LSTM.\n",
    "model_lstm = LSTMLinear(input_dim, output_dim, hidden_size, n_layers).to(device)\n",
    "\n",
    "# Transférer le modèle au bon matériel\n",
    "model_lstm = model_lstm.to(device)\n",
    "\n",
    "# Sauvegarder les poids initiaux du modèle.\n",
    "init_lstm_weights = copy.deepcopy(model_lstm.state_dict())\n",
    "\n",
    "# Transférer les données au bon matériel\n",
    "Xs = Xs.to(device)\n",
    "\n",
    "# Utiliser le LSTM pour faire la prédiction de tous les exemples sans entraînement(c.à.d. sur les n_samples)\n",
    "# Vérifiez que les inputs d'entrée et sorties sont justes\n",
    "y_pred = model_lstm(X)\n",
    "print(\"Dimensions initiales des données en entrée: {}\".format(X.shape)) # (seq_len, batch_size, input_size)\n",
    "print(\"Dimensions des prédictions: {}\".format(y_pred.shape)) # (batch_size, input_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gdha8WWWhPXp"
   },
   "source": [
    "## Préparation des données\n",
    "\n",
    "### Exercice\n",
    "\n",
    "Nous allons préparer nos objets DataLoader pour gérer efficacement les ensembles de données que nous allons générer. \n",
    "\n",
    "Pour cet exemple, nous utilisons 20,000 séquences où 80% des données sert comme ensemble d'entraînement, 10% comme ensemble de validation et 10% comme ensemble test. Nous pouvons utiliser les fonctions [torch.utils.data.TensorDataset()](https://pytorch.org/docs/stable/data.html) et [torch.utils.data.DataLoader()](https://pytorch.org/docs/stable/data.html) afin de préparer les Dataloader.\n",
    "\n",
    "Utilisez les valeurs suivantes:\n",
    "\n",
    "`seq_len = 18` \n",
    "\n",
    "`batch_size = 64`\n",
    "\n",
    "`n_samples = 25000`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [],
    "colab": {},
    "colab_type": "code",
    "id": "PzSCKyyMhPXq"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "n_samples = 25000\n",
    "seq_len = 18\n",
    "batch_size = 64\n",
    "\n",
    "# À Compléter\n",
    "\n",
    "X, Y = ... # Générer les données standardisées\n",
    "\n",
    "\n",
    "# À Compléter\n",
    "\n",
    "xtrain, ytrain = X[:round(0.8*n_samples)], Y[:round(0.8*n_samples)]\n",
    "xvalid, yvalid = ...\n",
    "xtest, ytest = ...\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(xtrain, ytrain),\n",
    "                          batch_size, shuffle=True)\n",
    "\n",
    "valid_loader = ...\n",
    "test_loader = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "W8IdyMkZjRs7"
   },
   "source": [
    "## Entraînement du RNN\n",
    "\n",
    "De nombreuses fonctions de coût et optimiseurs sont disponibles dans PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N-KSQ72HhPXt"
   },
   "source": [
    "### Exercice: Définir la fonction de coût et l'optimiseur\n",
    "\n",
    "Rappelons qu'une fonction de coût $J(\\theta) = L(x, y, \\theta)$ prend en entrée le tuple (prédiction, cible) et calcule une valeur qui estime la distance entre la prédiction et la cible. L'optimiseur que nous utilisons dans cet exemple est celui de la descente de gradient stochastique (*stochastic gradient descent* (SGD)), minimise la fonction de coût $J(\\theta)$ paramétrisée par les poids du modèle $\\theta$ en mettant à jour les poids itérativement suivant la règle de mise à jour suivante: $\\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta)$, où  $\\alpha$ est le taux d'apprentissage (*learning rate*).\n",
    "\n",
    "Pour un problème de régression comme nous avons dans cet exemple, un choix commun est d'utiliser les fonctions suivantes :\n",
    "<ul>\n",
    "<li>**Fonction de coût :** <a href=\"https://pytorch.org/docs/stable/nn.html\">`torch.nn.MSELoss()`</a>. L'erreur quadratique moyenne permet de calculer la moyenne de l'écart au carré entre la valeur prédite et la valeur désirée. Cette fonction est définie comme:\n",
    "\n",
    "$J(\\cdot) = \\frac{1}{N}\\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^{2}$.\n",
    "    \n",
    "<li>**Optimiseur :** <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD()`</a> qui est une implémentation de Stochastic Gradient Descent.</li>\n",
    "</ul>\n",
    "\n",
    "Nous utilisons une valeur de taux d'apprentissage de 0.01."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6SidKvG6hPXu"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.01\n",
    "criterion = ... # à compléter\n",
    "optimizer = ... # à compléter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pBrLb8PdhPXw"
   },
   "source": [
    "### Exercice: Entraînement du modèle\n",
    "\n",
    "Nous utilisons notre objet `train_loader` pour itérer à travers notre ensemble d'entraînement *n_epoch* fois pour faire l'entraînement du modèle. La valeur accumulée de la fonction de coût évaluée sur l'ensemble de validation est sauvegardée à la fin de chaque *epoch* d'entraînement.Nous utiliserons `n_epoch = 25`.\n",
    "\n",
    "Complétez le code suivant avec les instructions correspondant aux commentaires correspondants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e9a-gjjVhPXx"
   },
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "n_epoch = 25\n",
    "\n",
    "model_rnn.load_state_dict(init_rnn_weights)\n",
    "\n",
    "print(\"Début de l'entraînemetn\")\n",
    "\n",
    "for epoch in range(n_epoch):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_n_iter = 0\n",
    "    \n",
    "    # Mettre le modèle en mode d'entraînement\n",
    "    ....\n",
    "    \n",
    "    \n",
    "    # Itérer sur les données d'entraînement\n",
    "    for x, y in train_loader:  \n",
    "\n",
    "        \n",
    "        # Mettre les tenseurs sur le matériel (GPU si possible)\n",
    "        ...\n",
    "\n",
    "\n",
    "        # Réinitisaliser les gradients à zéro\n",
    "        ...\n",
    "        \n",
    "        # Exécuter la propagation avant\n",
    "        outputs = ...\n",
    "        \n",
    "        # Calculer la perte avec le critère\n",
    "        loss = criterion(...)\n",
    "        \n",
    "        # Exécuter la propagation arrière\n",
    "        ...\n",
    "        \n",
    "        # Exécuter le pas d'optimisation\n",
    "        ...\n",
    "        \n",
    "        # Statistiques\n",
    "        train_loss += loss.item()\n",
    "        train_n_iter += 1\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_n_iter = 0\n",
    "    \n",
    "    # Mettre le modèle en mode d'évaluation\n",
    "    ...\n",
    "    \n",
    "    # Itérer sur les données de validation\n",
    "    for x, y in valid_loader:  \n",
    "        \n",
    "        # Mettre les tenseurs sur le matériel (GPU si possible)\n",
    "        ...\n",
    "        \n",
    "        # Exécuter la propagation avant\n",
    "        outputs = ...\n",
    "        \n",
    "        # Calculer la perte avec le critère\n",
    "        loss = criterion(...)\n",
    "        \n",
    "        # Statistiques\n",
    "        valid_loss += loss.item()\n",
    "        valid_n_iter += 1\n",
    "    \n",
    "    train_loss_history.append(train_loss / train_n_iter)\n",
    "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
    "\n",
    "\n",
    "    print(\"Époque {:2d} | Perte d'entraînement = {:.5f} | Perte de validation = {:.5f} \"\n",
    "          .format(epoch+1, (train_loss / train_n_iter), (valid_loss / valid_n_iter)))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "\n",
    "print('\\n\\nEntraînement fini en {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wr4at5nVhPXz"
   },
   "source": [
    "### Visualisation des courbes d'entraînement\n",
    "\n",
    "Visualisez les courbes d'entraînement avec un graphique mettant en valeur la fonction de cout vs. le nombre d'époque pour l'ensemble d'entraînement et de validation sur un même graphe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bJLu-rC_hPX1"
   },
   "outputs": [],
   "source": [
    "# Save history for later\n",
    "rnn_train_loss_history = train_loss_history\n",
    "rnn_valid_loss_history = valid_loss_history\n",
    "\n",
    "# Plot training and validation curve\n",
    "xaxis = range(1, n_epoch + 1)\n",
    "plt.plot(xaxis, rnn_train_loss_history, label='entraînement-rnn')\n",
    "plt.plot(xaxis, rnn_valid_loss_history, label='validation-rnn')\n",
    "\n",
    "plt.xlabel('# époques')\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vi7bHNzykdby"
   },
   "source": [
    "## Entraînement LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QoahNuS73snl"
   },
   "source": [
    "### Exercice: Définir la fonction de coût et l'optimiseur\n",
    "\n",
    "Rappelons qu'une fonction de coût $J(\\theta) = L(x, y, \\theta)$ prend en entrée le tuple (prédiction, cible) et calcule une valeur qui estime la distance entre la prédiction et la cible. L'optimiseur que nous utilisons dans cet exemple est celui de la descente de gradient stochastique (*stochastic gradient descent* (SGD)), minimise la fonction de coût $J(\\theta)$ paramétrisée par les poids du modèle $\\theta$ en mettant à jour les poids itérativement suivant la règle de mise à jour suivante: $\\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta)$, où  $\\alpha$ est le taux d'apprentissage (*learning rate*).\n",
    "\n",
    "Pour un problème de régression comme nous avons dans cet exemple, un choix commun est d'utiliser les fonctions suivantes :\n",
    "<ul>\n",
    "<li>**Fonction de coût :** <a href=\"https://pytorch.org/docs/stable/nn.html\">`torch.nn.MSELoss()`</a>. L'erreur quadratique moyenne permet de calculer la moyenne de l'écart au carré entre la valeur prédite et la valeur désirée. Cette fonction est définie comme:\n",
    "\n",
    "$J(\\cdot) = \\frac{1}{N}\\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^{2}$.\n",
    "    \n",
    "<li>**Optimiseur :** <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD()`</a> qui est une implémentation de SGD.</li>\n",
    "</ul>\n",
    "\n",
    "Nous utilisons une valeur de taux d'apprentissage de 0.001.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pOZIqkBbpmeo"
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "# Définir le critère\n",
    "criterion = ...\n",
    "\n",
    "# Définir l'optimiseur\n",
    "optimizer = ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qqnTlRq8pmew"
   },
   "source": [
    "### Exercice: Entraînement du model\n",
    "\n",
    "Nous utilisons notre objet `train_loader` pour itérer à travers notre ensemble d'entraînement *n_epoch* fois pour faire l'entraînement du modèle. La valeur accumulée de la fonction de coût évaluée sur l'ensemble de validation est sauvegardée à la fin de chaque *epoch* d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "odcwM6DyhPYL"
   },
   "outputs": [],
   "source": [
    "since = time.time()\n",
    "\n",
    "\n",
    "train_loss_history = []\n",
    "valid_loss_history = []\n",
    "\n",
    "num_epochs = 25\n",
    "\n",
    "model_lstm.load_state_dict(init_lstm_weights)\n",
    "\n",
    "print(\"# Début de l'entraînement #\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    train_loss = 0\n",
    "    train_n_iter = 0\n",
    "    \n",
    "    # Mettre le modèle en mode d'entraînement\n",
    "    ...\n",
    "    \n",
    "    # Itérer sur les données d'entraînement\n",
    "    for x, y in train_loader:  \n",
    "\n",
    "        \n",
    "        # Mettre les tenseurs sur le matériel (GPU si possible)\n",
    "        ...\n",
    "\n",
    "        # Réinitisaliser les gradients à zéro\n",
    "        ...\n",
    "        \n",
    "        # Exécuter la propagation avant\n",
    "        outputs = ...\n",
    "        \n",
    "        # Calculer la perte avec le critère\n",
    "        loss = criterion(...)\n",
    "        \n",
    "        # Exécuter la propagation arrière\n",
    "        ...\n",
    "        \n",
    "        # Exécuter le pas d'optimisation\n",
    "        ...\n",
    "        \n",
    "        # Statistiques\n",
    "        train_loss += loss.item()\n",
    "        train_n_iter += 1\n",
    "    \n",
    "    valid_loss = 0\n",
    "    valid_n_iter = 0\n",
    "    \n",
    "    # Mettre le modèle en mode d'évaluation\n",
    "    ...\n",
    "    \n",
    "        # Itérer sur les données de validation\n",
    "    for x, y in valid_loader:  \n",
    "        \n",
    "        # Mettre les tenseurs sur le matériel (GPU si possible)\n",
    "        ...\n",
    "        # Exécuter la propagation avant\n",
    "        \n",
    "        outputs = ...\n",
    "        \n",
    "        # Calculer la perte avec le critère\n",
    "        loss = ...\n",
    "        \n",
    "        # Statistiques\n",
    "        valid_loss += loss.item()\n",
    "        valid_n_iter += 1\n",
    "    \n",
    "    train_loss_history.append(train_loss / train_n_iter)\n",
    "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
    "    \n",
    "    print(\"Époque {:2d} | Perte d'entraînement = {:.5f} | Perte de validation = {:.5f} \"\n",
    "          .format(epoch+1, (train_loss / train_n_iter), (valid_loss / valid_n_iter)))\n",
    "\n",
    "time_elapsed = time.time() - since\n",
    "\n",
    "print('\\n\\nEntraînement fini en {:.0f}m {:.0f}s'.format(\n",
    "    time_elapsed // 60, time_elapsed % 60))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aMEzsqBaqGmh"
   },
   "source": [
    "### Visualisation des courbes d'entraînement\n",
    "\n",
    "Visualisez les courbes d'entraînement avec un graphique mettant en valeur la fonction de coût vs. le nombre d'époques pour l'ensemble d'entraînement et de validation sur un même graphe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Rw1tIfMhPYN"
   },
   "outputs": [],
   "source": [
    "# Save history for later\n",
    "lstm_train_loss_history = train_loss_history\n",
    "lstm_valid_loss_history = valid_loss_history\n",
    "\n",
    "# Plot training and validation curve\n",
    "xaxis = range(1, num_epochs + 1)\n",
    "plt.plot(xaxis, lstm_train_loss_history, label='entraînement-lstm')\n",
    "plt.plot(xaxis, lstm_valid_loss_history, label='validation-lstm')\n",
    "\n",
    "plt.xlabel('# époques')\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7PCowiQxpcHv"
   },
   "source": [
    "## Analyse des résultats\n",
    "\n",
    "Analysons l'erreur générée par notre modèle sur notre ensemble test.  Nous allons comparer l'erreur quadratique moyenne sur l'ensemble test pour le modèle RNN et LSTM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KaoQgOOZKL98"
   },
   "source": [
    "### Exercice: Comparaisons des courbes d'entraînenement\n",
    "\n",
    "Nous allons comparer sur un même graphique l'erreur en fonction du nombre d'epochs lors de notre entraînement sur nos ensembles d'entraînement et de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yl8hcNhnKKOS"
   },
   "outputs": [],
   "source": [
    "# Plot training and validation curve\n",
    "xaxis = range(1, num_epochs + 1)\n",
    "\n",
    "plt.plot(xaxis, rnn_train_loss_history, label='entraînement-rnn')\n",
    "plt.plot(xaxis, rnn_valid_loss_history, label='validation-rnn')\n",
    "\n",
    "plt.plot(xaxis, lstm_train_loss_history, label='entraînement-lstm', linestyle='--')\n",
    "plt.plot(xaxis, lstm_valid_loss_history, label='validation-lstm', linestyle='--')\n",
    "\n",
    "plt.xlabel('# époques')\n",
    "plt.ylabel('Perte')\n",
    "plt.legend()\n",
    "plt.ylim([0,2])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "BhGBAmabKr8A"
   },
   "source": [
    "Remarquez qu'il y a une différence entre les résultats des RNN et des LSTM.\n",
    "\n",
    "**Questions**\n",
    "\n",
    "1.   Quel réseau fonctionne mieux sur notre ensemble validation? Pourquoi?\n",
    "2.   Supposons que vous vouliez augmenter l'écart entre les performances du RNN et du LSTM. Comment modifieriez-vous les données ?\n",
    "\n",
    "... # à compléter.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SeWGSfEOLdsM"
   },
   "source": [
    "### Exercice: Calcul de l'erreur sur l'ensemble test\n",
    "\n",
    "Calculez l'erreur quadratique moyenne sur l'ensemble test pour les RNN et les LSTM et imprimez les deux valeurs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "72z2Xob3MRYB"
   },
   "outputs": [],
   "source": [
    "# Mettre les données séquentielles sur le matériel (GPU si possible)\n",
    "xtest = ...\n",
    "\n",
    "# Mettre les étiquettes sur le matériel (GPU si possible)\n",
    "ytest = ...\n",
    "\n",
    "# Calculer les valeurs prédites par les RNN et LSTM pour l`ensemble test\n",
    "\n",
    "ypred_rnn = ...\n",
    "ypred_lstm = ...\n",
    "\n",
    "\n",
    "# Calculer la perte du modèle à base de RNN avec le critère\n",
    "loss_test_rnn = ...\n",
    "\n",
    "# Calculer la perte du modèle à base de LSTM avec le critère\n",
    "loss_test_lstm = ...\n",
    "\n",
    "print(\"L'erreur quadratique moyenne des RNN sur l'ensemble test (standardisé) est de %2.3f\" % float(loss_test_rnn))\n",
    "print(\"L'erreur quadratique moyenne des LSTM sur l'ensemble test (standardisé) est de %2.3f\" % float(loss_test_lstm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hPZH71eCpcH0"
   },
   "source": [
    "### Exercice: Vérification visuelle des résultats\n",
    "\n",
    "Pour mieux comprendre nos modèles, nous examinerons leurs résultats (c'est-à-dire leurs prédictions avant de calculer le coût). Nous définissons une fonction `print_sequence()` qui aidera à visualiser les prédictions faites par nos modèles. \n",
    "\n",
    "Cette fonction prend en entrée les tenseurs X et Y pour échantillonner et imprimer un exemple de séquence ainsi que la différence absolue entre une valeur de Y et la valeur réelle de la somme de X.\n",
    "\n",
    "Rappelons que nous avons standardisé nos exemples. Pour ces visualisations, nous souhaitons utiliser la pré-normalisation des données originales. Pour ce faire, il faut réutiliser `mean` + `std` tel que \n",
    "\n",
    "$xtest\\_unstd = xtest*std + mean$ et $ytest\\_unstd = ytest*std+seq\\_len*mean$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QfCvsGXNpcH2"
   },
   "outputs": [],
   "source": [
    "def print_sequence(X, Y, idx=0):\n",
    "    \"\"\"Imprimer et vérifier les séquences d'opérations.\n",
    "    Args:\n",
    "      X: torch.Tensor.\n",
    "      Y: torch.Tensor.\n",
    "      idx: indice de la séquence à vérifier.\n",
    "    \"\"\"\n",
    "    x = X[idx].numpy()\n",
    "    y = Y[idx].numpy()\n",
    "    for i, xi in enumerate(x):\n",
    "        if i==0:\n",
    "            string = str(xi[0]) \n",
    "        else:\n",
    "            string += \" + \" + str(xi[0])\n",
    "    string1 = string + \" = \" + str(y[0])\n",
    "    string2 = string + \" = \" + str(np.sum(x))\n",
    "    print(\"Prédiction: \", string1)\n",
    "    print(\"Valeur réelle: \", string2)\n",
    "    ecart = abs(np.sum(x)-y[0])\n",
    "    print(\"Écart absolu entre X[{a}] et Y[{a}]: {b}\".format(a=idx, b=ecart))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Complétez le morceau de code suivant en déstandardisant les valeurs `xtest` et `ypred_rnn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_aDw139hpcH_"
   },
   "outputs": [],
   "source": [
    "xtest = xtest.cpu()\n",
    "ypred_rnn = ypred_rnn.cpu()\n",
    "\n",
    "xtest_unstd = ...\n",
    "ypred_unstd = ...\n",
    "\n",
    "idx = np.random.randint(len(ytest))\n",
    "\n",
    "\n",
    "print(\"\")\n",
    "print(\"Exemple RNN:\")\n",
    "print(\"\")\n",
    "\n",
    "print_sequence(xtest_unstd.detach(), ypred_unstd.detach(), idx=idx)\n",
    "\n",
    "\n",
    "\n",
    "xtest = xtest.cpu()\n",
    "ypred_lstm = ypred_lstm.cpu()\n",
    "\n",
    "xtest_unstd = xtest*std + mean\n",
    "ypred_unstd = ypred_lstm*std + seq_len*mean\n",
    "\n",
    "print(\"\")\n",
    "print(\"Exemple LSTM:\")\n",
    "print(\"\")\n",
    "\n",
    "print_sequence(xtest_unstd.detach(), ypred_unstd.detach(), idx=idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tAM5UeILQb5l"
   },
   "source": [
    "** Questions bonus**\n",
    "\n",
    "* Que pouvez-vous suggérer comme techniques et modifications pour améliorer la performance des LSTM?\n",
    "* Répétez l'exercice pour différentes opérations mathématiques (+, -, x, /, ...)\n",
    "* Faites un graphique comparant la performance des RNN et LSTM en fonction de `seq_len`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y1jHLBHErzbW"
   },
   "source": [
    "---\n",
    "# Tâche 2: Modèle de langue neuronal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h7kMH3bm1ZDz"
   },
   "source": [
    "## Objectif\n",
    "L'objectif de la deuxième partie du tutoriel est d'apprendre à générer des textes à l'aide de réseaux de neurones récurrents. En particulier, nous allons entraîner un réseau de neurones récurrents en utilisant une petite quantité de données textuelles écrites par [Shakespeare]https://en.wikipedia.org/wiki/William_Shakespeare). Une fois ce modèle entraîné, nous l'utiliserons pour générer un nouveau texte dans le style de Shakespeare.\n",
    "\n",
    "**Notez que les gens utilisent souvent le terme RNN même lorsqu'ils entraînent un LSTM. RNN est devenu le terme générique, quel que soit le type (outre les LSTM, il existe d'autres variantes couramment utilisées, telles que [Gated Recurrent Units (GRUs)] (http://colah.github.io/posts/2015-08-Understanding-LSTMs/)).\n",
    "\n",
    "Le modèle que nous utiliserons est un peu plus complexe que celui de la tâche 1. Contrairement à la section précédente, tout le code est fourni (c'est-à-dire qu'il n'y a pas d'exercices à compléter). Nous vous suggérons de parcourir l'ensemble du code pour vous assurer que vous comprenez à la fois sa logique en ce qui concerne le traitement de texte mais aussi comment concevoir et entraîner un RNN pour la génération de texte. Le code pourrait être relativement facilement adapté à d'autres tâches qui pourraient vous intéresser. \n",
    "\n",
    "En outre, ce cahier contient également certaines des étapes de prétraitement des données, en particulier celles qui consistent à prendre notre ensemble de données et à l'organiser de manière à ce qu'il puisse être utilisé pour entraîner un modèle de langage neural basé sur les LSTM.  \n",
    "\n",
    "Bonne génération !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-TECjbTm3L20"
   },
   "source": [
    "## Encore un peu de contexte théorique\n",
    "\n",
    "Une séquence de mots $\\mathbf{s}$ peut être représentée comme une séquence de symboles discrets $N$ (ou jetons lexicaux) telle que $\\mathbf{s} = (w_{1}, \\dots, w_{N})$, où $w_{t}$ est un mot ou un signe de ponctuation. Chaque symbole peut être représenté par un entier correspondant à son indice dans le vocabulaire $V$. $V$ contient tous les symboles d'une tâche particulière (le vocabulaire est généralement construit à partir de l'ensemble de données que nous utilisons pour une tâche). L'objectif d'un modèle de langage est d'estimer la probabilité (conjointe) d'une séquence $p(\\mathbf{s}) = p(w_{1}, \\dots, w_{N})$, qui peut être décomposée comme un produit de probabilités conditionnelles telles que :\n",
    "\n",
    "\\begin{equation}\n",
    "  p(\\mathbf{s}) = \\prod^{N}_{t=1} p(w_{t} | w_{1}, \\dots, w_{t-1}).\n",
    "\\end{equation}\n",
    "(ceci est également connu comme la règle de la chaîne dans la théorie des probabilités)\n",
    "\n",
    "C'est important pour la modélisation. En particulier, au lieu de modéliser directement la distribution conjointe, nous pouvons \"simplement\" modéliser chaque conditionnel. C'est-à-dire que nous pouvons modéliser la probabilité du mot suivant compte tenu de tous les mots précédents ($p(w_{t} | w_{<t})$). C'est précisément ce que font les modèles linguistiques, qui sont largement utilisés dans de nombreuses applications (notamment dans la traduction automatique, la reconnaissance vocale et la recherche d'informations). Notez que cela peut être compris comme un problème de classification multi-classes où les classes correspondent aux différents mots.\n",
    "\n",
    "Cependant, la modélisation de chaque condition n'est pas facile. Nous effectuons plutôt une approximation qui rend le problème plus facile. L'intuition derrière l'approximation est qu'au lieu de conditionner l'histoire entière, nous conditionnons sur une histoire plus petite ($w_{t-1}, w_{t-2}, \\ldots, w_{t-n}$) pour prédire le mot suivant ($w_{t}$). C'est ce qu'on appelle une hypothèse de Markov d'ordre $n$. Mathématiquement, c'est le cas :\n",
    "\\begin{equation}\n",
    "  p(w_{t} | w_{1}, \\dots, w_{t-1}) \\approx p(w_{t} | w_{t-n}, \\dots, w_{t-1}).\n",
    "\\end{equation}\n",
    "\n",
    "Dans le paragraphe suivant, nous expliquons comment modéliser les conditions ci-dessus en utilisant un réseau neuronal récurrent. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modélisation à l'aide des RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'architecture du modèle de langage neuronal que nous utilisons est un réseau LSTM qui apprendra à chaque pas de temps une distribution conditionnelle du mot suivant en utilisant un certain nombre de mots précédents.\n",
    "Pour entraîner ses paramètres, nous devons d'abord fixer le nombre maximum $n$ de mots précédents à prendre en compte (`seq_len` dans le code)-- c'est la taille effective de votre historique -- pour l'entraînement $p(w_{t} | w_{t-n}, \\dots, w_{t-1})$. L'entrée dans le LSTM à chaque étape de temps est : \n",
    "- le mot $t^{th}$ $w_t$ encodé à l'aide de son *mot incorporé* (voir ci-dessous) ; \n",
    "- l'état récurrent ($\\mathbf{h}_{t-1}$) ; et \n",
    "- l'état de la mémoire au pas de temps précédent ($\\mathbf{c}_{t-1}$).\n",
    "\n",
    "La sortie du LSTM à chaque étape est le mot suivant $w_{t+1}$. C'est-à-dire que nous entraînons le LSTM à prédire le mot suivant à chaque étape (en détail, le LSTM prédira en fait la probabilité de ce mot suivant). Cela implique également qu'une fois ce modèle entraîné, nous pourrons l'utiliser pour générer du texte (nous choisirons simplement le mot ayant la plus grande probabilité et l'introduirons comme entrée à l'étape suivante). Ce type de modèle est communément connu sous le nom de *modèle de réseau neuronal basé sur LSTM*. Son architecture est présentée ci-dessous :\n",
    "\n",
    "![alt-text](https://github.com/nextai-mtl/tech-2019/blob/master/images/autoregressive_english.png?raw=true)\n",
    "\n",
    "\n",
    "Pour calculer la probabilité sur tous les mots suivants, nous utilisons simplement une fonction d'activation *softmax*. La fonction softmax renvoie un vecteur normalisé de dimension $|V|$, où chaque entrée correspond à un seul mot du vocabulaire. Chaque entrée peut être comprise comme la \"probabilité\" que le mot suivant soit le mot à cet indice.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plongement de mots"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La principale question qui reste est de savoir comment encoder exactement les mots à chaque pas de temps. \n",
    "Comme les données d'entrée d'un réseau neuronal doivent pouvoir être encodées dans une matrice, chaque symbole (mot) $w_t$ du vocabulaire peut être représenté par un vecteur *un-échauffé* $\\mathbf{x}_i$ qui est un vecteur de zéros avec un seul 1 à la position de l'index de ce mot dans le vocabulaire. Ainsi, ces vecteurs *one-hot* appartiennent à $ \\mathbb R^{|V|}$ où $|V|$ est la taille du vocabulaire, c'est-à-dire le nombre de mots dans le vocabulaire. Ces vecteurs one-hot sont multipliés par une matrice de pondération $ \\mathbf{E} \\in \\mathbb R^{|V| \\times d_{e}}$. Cette matrice est apprise et est connue sous le nom de *matrice d'incorporation*, $\\mathbf{E} \\in \\mathbb R^{|V| \\times d_{e}}$, il encode effectivement les mots dans une représentation continue : $\\mathbf{w}_{t} \\in \\mathbb R^{d_{e}}$.\n",
    "\n",
    "En utilisant des encastrements de mots, une séquence de mots peut donc être représentée par une séquence de vecteurs de taille $N$ $\\mathbf{s} = (\\mathbf{w}_{1}, \\dots, \\mathbf{w}_{N})$. Chaque ligne $i$ de cette matrice $\\mathbf{E}$ est une représentation en dimensions $d_{e}$ du $i$'ème mot du vocabulaire $V$. Comme nous l'avons dit plus haut, ces représentations sont souvent appelées *intégrations de mots*. Lorsqu'elles sont apprises à partir d'un ensemble de données suffisamment important, elles peuvent représenter une similarité sémantique. [Pour plus d'informations sur l'intégration des mots] (http://ruder.io/word-embeddings-1/index.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1Sh2OG46_Ln"
   },
   "source": [
    "## Fonctions utilitaires de traitement de texte et structuration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sf7luc-Di7qO"
   },
   "source": [
    "Pour entraîner notre modèle de langage neuronal, nous utiliserons un ensemble de données (corpus) des travaux de Shakespeare (nous avons utilisé un sous-ensemble de ce lien [https://norvig.com/ngrams/] qui a été nettoyé, tokenisé et standardisé) disponible dans le fichier `shakespeare_top20K.txt`.\n",
    "\n",
    "Pour obtenir des résultats raisonnables dans la pratique, nous devrions entraîner un modèle linguistique en utilisant une très grande quantité de texte. Dans ce tutoriel, nous utiliserons un corpus relativement petit de 20000 phrases, 159884 jetons, et un vocabulaire de 12354 jetons. Un token est une unité lexicale séparée par un espace de chaque côté du texte. Dans notre cas, un jeton est un mot, un nombre ou un signe de ponctuation. Le vocabulaire est l'ensemble de tous les jetons d'un corpus. (Bien entendu, notre code se généralise à des ensembles de données plus importants)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yo7yRlDyK7DA"
   },
   "outputs": [],
   "source": [
    "# Cloner git repos pour accéder aux données\n",
    "!git clone https://github.com/nextai-mtl/tech-2019.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0pjv24-kr528"
   },
   "outputs": [],
   "source": [
    "START_VOCAB = [\"_UNK\"]\n",
    "UNK_ID = 0\n",
    "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `create_vocabulary` prend comme entrée :\n",
    "- **corpus_path** : chemin vers les données d'un corpus\n",
    "- **vocab_path** : le chemin où un vocabulaire du corpus fourni sera créé\n",
    "- **max_vocab_size** : le nombre maximum de mots du vocabulaire\n",
    "\n",
    "Il crée un vocabulaire composé au maximum de `max_vocab_size` des jetons fréquents dans les données du corpus. Ce vocabulaire est enregistré sous le fichier `vocab_path`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sdH5sXVsDgU"
   },
   "outputs": [],
   "source": [
    "def create_vocabulary(corpus_path, vocab_path, max_vocab_size=1e5):\n",
    "    \"\"\"Créer et sauvegarder le vocabulaire d'un corpus.\"\"\"\n",
    "    vocab = {}\n",
    "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            for token in tokens:\n",
    "                if token in vocab:\n",
    "                    vocab[token] += 1\n",
    "                else:\n",
    "                    vocab[token] = 1\n",
    "    vocab_list = START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
    "    if len(vocab_list) > max_vocab_size:\n",
    "        vocab_list = vocab_list[:max_vocab_size]\n",
    "    with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        for token in vocab_list:\n",
    "              f.write(token + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction \" initialize_vocabulary \" prend comme entrée :\n",
    "- **vocab_path** : le chemin d'accès au fichier contenant le vocabulaire\n",
    "\n",
    "Il renvoie un tuple de deux éléments correspondant respectivement à :\n",
    "- **vocab** : un dictionnaire `Token:Index` associant un index à chaque token du vocabulaire\n",
    "- **rev_vocab** : une liste de jetons (uniques) dans le vocabulaire qui associe chaque index à un jeton tel que $vocab [rev\\_vocab[i]] = i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sdH5sXVsDgU"
   },
   "outputs": [],
   "source": [
    "def initialize_vocabulary(vocab_path):\n",
    "    \"\"\"Initialiser le vocabulaire.\"\"\"\n",
    "    if os.path.exists(vocab_path):\n",
    "        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            rev_vocab = [line.strip() for line in f.readlines()]\n",
    "        vocab = dict([(w, i) for (i, w) in enumerate(rev_vocab)])\n",
    "        return vocab, rev_vocab\n",
    "    else:\n",
    "        raise ValueError(\"Vocabulary file {} not found.\".format(vocab_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `read_corpus` prend comme entrée :\n",
    "- **corpus_path** : chemin d'accès aux données d'un corpus\n",
    "\n",
    "Il renvoie les données du corpus sous la forme d'une liste de jetons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sdH5sXVsDgU"
   },
   "outputs": [],
   "source": [
    "def read_corpus(corpus_path):\n",
    "    \"\"\"Lire et convertir un corpus en une liste de tokens.\"\"\"\n",
    "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        corpus = f.read().split()\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `corpus_to_token_ids` prend comme entrée :\n",
    "- **corpus** : le corpus comme une liste de jetons\n",
    "- **vocab** : le vocabulaire sous forme de distionnaire de la forme \"taken:index\".\n",
    "\n",
    "Il renvoie une liste de token-ids correspondant à la liste de token telle que définie par le corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sdH5sXVsDgU"
   },
   "outputs": [],
   "source": [
    "def corpus_to_token_ids(corpus, vocab):\n",
    "    \"\"\"Convertir un corpus en token-ids.\"\"\"\n",
    "    token_ids = [vocab.get(token, UNK_ID) for token in corpus]\n",
    "    return token_ids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `batch_data` prend comme entrée :\n",
    "- **data** : liste des token_ids\n",
    "- **batch_size** : la taille du lot\n",
    "\n",
    "Il renvoie une version structurée des données en séquences continues de la taille d'un lot. Les données renvoyées ont une forme `N* x batch_size` où `N* = int(len(data) / batch_size)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sdH5sXVsDgU"
   },
   "outputs": [],
   "source": [
    "def batch_data(data, batch_size):\n",
    "    \"\"\"Structurer les données en batch_size séquences continues.\"\"\"\n",
    "    n_batch = len(data) // batch_size\n",
    "    data = np.array(data[:n_batch*batch_size])\n",
    "    data = data.reshape(batch_size, -1).T\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La fonction `detach_hidden` prend comme entrée :\n",
    "- **hidden** : état caché d'un RNN\n",
    "\n",
    "Il transforme les données de cet état caché en un nouveau tenseur avec les mêmes valeurs mais où l'historique de calcul est perdu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_sdH5sXVsDgU"
   },
   "outputs": [],
   "source": [
    "def detach_hidden(hidden):\n",
    "    \"\"\"Transformer les données des états cachés d'un LSTM en\n",
    "       nouveaux Tensor avec require_grad=False.\"\"\"\n",
    "    return tuple(h.detach() for h in hidden)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le morceau de code où les données du corpus sont lues, le vocabulaire est créé et les données du corpus sont structurées en séquences continues de \"batch_size\" à des fins d'entraînement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o0mwd1xGsFM2"
   },
   "outputs": [],
   "source": [
    "# Créer et initialiser le vocabulaire.\n",
    "corpus_path = \"./ecole_dl_mila_ivado/tutoriaux/RNN/data/voltaire.txt\"\n",
    "vocab_path = \"vocab.txt\"\n",
    "\n",
    "create_vocabulary(corpus_path, vocab_path)\n",
    "vocab, rev_vocab = initialize_vocabulary(vocab_path)\n",
    "\n",
    "# Lire le corpus d'entraînement.\n",
    "corpus = read_corpus(corpus_path)\n",
    "token_ids = corpus_to_token_ids(corpus, vocab)\n",
    "\n",
    "# Structurer le corpus en batch_size séquences continues pour faire l'entraînement.\n",
    "batch_size = 10\n",
    "data = batch_data(token_ids, batch_size)\n",
    "data = torch.LongTensor(data).to(DEVICE)\n",
    "\n",
    "print(\"Nombre de tokens dans le corpus: {}\"\n",
    "      .format(len(corpus)), end=\"\\n\\n\")\n",
    "print(\"Taille du vocabulaire: {}\"\n",
    "      .format(len(vocab)), end=\"\\n\\n\")\n",
    "print(\"Liste des 20 tokens les plus fréquents dans le corpus: \\n{}\"\n",
    "      .format(rev_vocab[1:21]), end=\"\\n\\n\")\n",
    "print(\"Première phrase du corpus en format texte:\\n{}\"\n",
    "      .format(\" \".join(corpus[:31])), end=\"\\n\\n\")\n",
    "print(\"Première phrase du corpus transformée en token-ids:\\n{}\"\n",
    "      .format(token_ids[:31]), end=\"\\n\\n\")\n",
    "print(\"Test de conversion token-ids vers texte en utilisant rev_vocab:\\n{}\"\n",
    "      .format(\" \".join([rev_vocab[i] for i in token_ids[:31]])), end=\"\\n\\n\")\n",
    "print(\"Structure des données d'entraînement (observez que la première phrase \"\n",
    "      \"est dans la première colonne):\\n{}\".format(data[:20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans le bloc suivant, nous divisons les données en ensembles de données d'entraînement et de validation et nous créons les collecteurs de données correspondants. Nous utilisons 10 % des données pour l'ensemble de validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E2EXx8HBsZAY"
   },
   "outputs": [],
   "source": [
    "# Split ensembles training/validation et créer DataLoaders.\n",
    "X = data[:-1]\n",
    "Y = data[1:]\n",
    "\n",
    "n_valid = round(data.size(0) * 0.1)\n",
    "train_set = TensorDataset(X[:(data.size(0)-n_valid)], Y[:(data.size(0)-n_valid)])\n",
    "valid_set = TensorDataset(X[-n_valid:], Y[-n_valid:])\n",
    "\n",
    "seq_len = 40\n",
    "train_loader = DataLoader(train_set, batch_size=seq_len, shuffle=False)\n",
    "valid_loader = DataLoader(valid_set, batch_size=seq_len, shuffle=False)\n",
    "\n",
    "print(\"Ensemble d'entraînement: {} séquences de longueur {} et {} minibatches\"\n",
    "      .format(len(train_set), seq_len, len(train_loader)))\n",
    "print(\"Ensemble de validation : {} séquences de longueur {} et {} minibatches\"\n",
    "      .format(len(valid_set), seq_len, len(valid_loader)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IAgSaVwmVxrt"
   },
   "source": [
    "## Implémentation du modèle\n",
    "\n",
    "Ci-dessous se trouve le code du *LSTM-based neural language model* décrit dans la section **Encore un peu de contexte théorique (car vous aimez vraiment ça)**.\n",
    "\n",
    "À chaque pas temps les variables d'entrées dans le LSTM sont:\n",
    "\n",
    "1.    une minibatch de séquences de token-ids (c.à.d. des séquences d'indices où chaque indice représente la position d'un token dans un vocabulaire);\n",
    "2.    des tuples $(\\mathbf{h}_{0}, \\mathbf{c}_{0})$ des états récurrents et des états mémoires équivalent aux $(\\mathbf{h}_{T}, \\mathbf{c}_{T})$ de la minibatch précédente (sauf pour la première minibatch où nous initialisons les valeurs de ces variables à 0.0 avec la fonction `init_hidden()`).\n",
    "\n",
    "Chaque séquence de token-ids est transformée en séquence de *word embeddings* en indexant les représentations *word embeddings* créés par la classe [torch.nn.Embedding()](https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding) qui est une matrice de paramètres de dimension $|V| \\times d_{e}$, où $|V|$ est la taille du vocabulaire (`vocab_size`) et $d_{e}$ est la dimension d'un *word embedding* (`embedding_size`).\n",
    "\n",
    "Nous appliquons du dropout sur les *word embeddings* en entrée et sur la couche de sortie pour régulariser le modèle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0YMiwftasb4g"
   },
   "outputs": [],
   "source": [
    "class LanguageModel(nn.Module):\n",
    "    \"\"\"Modèle de langue neuronal à base de LSTM.\"\"\"\n",
    "    \n",
    "    def __init__(self, vocab_size, input_size, hidden_size, n_layers=1, dropout=0.5):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "          vocab_size: taille du vocabulaire.\n",
    "          input_size: taille des word embeddings.\n",
    "          hidden_size: taille des états cachés du LSTM.\n",
    "          n_layers: nombre de couches du LSTM (default: 1).\n",
    "          dropout: si non-zéro, introduit une couche dropout à l'entrée et à la sortie\n",
    "                   du LSTM, avec une probabilité égale à dropout (default: 0.5).\n",
    "        \"\"\"        \n",
    "        super(LanguageModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        self.embeddings = nn.Embedding(vocab_size, input_size)\n",
    "        self.lstm = nn.LSTM(input_size,\n",
    "                            hidden_size,\n",
    "                            n_layers)\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        self.init_weights()\n",
    "\n",
    "    def forward(self, input, hidden):\n",
    "        embeddings = self.dropout(self.embeddings(input))\n",
    "        output, hidden = self.lstm(embeddings, hidden)\n",
    "        output = self.dropout(output)\n",
    "        result = self.linear(output.view(output.size(0)*output.size(1), output.size(2)))\n",
    "        return result.view(output.size(0), output.size(1), result.size(1)), hidden\n",
    "\n",
    "    def init_weights(self):\n",
    "        init.uniform_(self.embeddings.weight, -0.1, 0.1)\n",
    "        init.xavier_uniform_(self.linear.weight, init.calculate_gain(\"linear\"))\n",
    "        init.constant_(self.linear.bias, 0)\n",
    "        \n",
    "    def init_hidden(self, batch_size):\n",
    "        \"\"\"Initialiser les valeurs de l'état caché et la cellule du LSTM à zéro.\n",
    "        Args:\n",
    "          batch_size: taille de la mini-batch à un pas de temps.\n",
    "          \n",
    "        Returns:\n",
    "          hidden: état caché h_t et la cellule c_t à t=0 initialisés à 0, \n",
    "                  ((n_layers, batch_size, hidden_size),\n",
    "                   (n_layers, batch_size, hidden_size)).\n",
    "        \"\"\"\n",
    "        hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_size, device=DEVICE),\n",
    "                  torch.zeros(self.n_layers, batch_size, self.hidden_size, device=DEVICE))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oPPkC_0GV3iu"
   },
   "source": [
    "## Entraînement du modèle\n",
    "L'entraînement du modèle se fait presque de la même manière que dans la tâche 1. Les éléments clés différents sont les suivants:\n",
    "* Nous appliquons du dropout sur les *word embeddings* en entrée et la couche de sortie du LSTM. Étant donné que la taille de notre ensemble d'entraînement est très petite, il est alors facile pour notre modèle de faire du surapprentissage. Pour les modéles génératifs autorégressifs à base de RNN, il est fortement recommandé d'utiliser une probabilité dropout élevée pour éviter l'effet de surapprentissage des données.\n",
    "* Nous démarrons l'entraînement avec une valeur de `learning_rate` élevée et nous la diminuons par un facteur de 10 en fonction de la perte sur l'ensemble de validation évaluée à la fin de chaque *epoch* d'entraînement en utilisant la classe [torch.optim.lr_scheduler.ReduceLROnPlateau()](https://pytorch.org/docs/stable/optim.html?highlight=plateau#torch.optim.lr_scheduler.ReduceLROnPlateau).\n",
    "* Pour éviter le *exploding gradient problem*, nous appliquons la technique de *gradient clipping* en normalisant la norme du gradient avec la fonction [torch.nn.utils.clip_grad_norm_()](https://pytorch.org/docs/stable/nn.html?highlight=clip#torch.nn.utils.clip_grad_norm_).\n",
    "* Nous initialisons avec des valeurs de 0.0 le tuple $(\\mathbf{h}_{0}, \\mathbf{c}_{0})$ de l'état récurrent et l'état mémoire `hidden` avec la fonction `init_hidden()` uniquement une fois au début de chaque *epoch* d'entraînement et nous propageons les nouvelles valeurs de `hidden` à travers chaque minibatch d'entraînement. En d'autres mots, les données ont été structurées à l'aide de la fonction `batch_data()` de manière à ce que nous pouvons initialiser `hidden` $(\\mathbf{h}_{0}, \\mathbf{c}_{0})$ de chaque séquence d'une nouvelle minibatch par $(\\mathbf{h}_{T}, \\mathbf{c}_{T})$ de la minibatch précédente. Un désavantage de cette méthode est que nous ne pouvons pas mélanger l'ordre des séquences à chaque *epoch* d'entraînement (c.à.d. `train_loader = DataLoader(train_set, batch_size=seq_len, shuffle=False)`).\n",
    "* Notre fonction de coût est l'entropie croisée [torch.nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/nn.html?highlight=crossentropy#torch.nn.CrossEntropyLoss).\n",
    "* Nous présentons la mesure de *[perplexity](https://en.wikipedia.org/wiki/Perplexity)* qui est une mesure d'évaluation de la qualité d'un modèle de langue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "winuGw1rsuOe"
   },
   "outputs": [],
   "source": [
    "# Construire le modèle.\n",
    "vocab_size = len(vocab)\n",
    "embedding_size = 300\n",
    "hidden_size = 400\n",
    "n_layers = 1\n",
    "dropout = 0.65\n",
    "model = LanguageModel(vocab_size, embedding_size, hidden_size, n_layers, dropout).to(DEVICE)\n",
    "print(\"Nombre de paramètres dans le modèle:\", sum(param.nelement() for param in model.parameters()))\n",
    "\n",
    "# Fonction de coût et optimiseur.\n",
    "learning_rate = 10\n",
    "loss_fun = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2, verbose=True)\n",
    "\n",
    "# Entraînement du modèle.\n",
    "n_epochs = 20\n",
    "max_grad_norm = 1\n",
    "\n",
    "print(\"Entraînement du modèle pour {} epochs de {} minibatches\".format(n_epochs, len(train_loader)))\n",
    "for epoch in range(n_epochs):\n",
    "    model.train()\n",
    "    hidden = model.init_hidden(batch_size)\n",
    "    train_loss = 0\n",
    "    valid_loss = 0\n",
    "    for x, y in train_loader:\n",
    "        # Détacher les états cachés précédents du graphe computationnel.\n",
    "        hidden = detach_hidden(hidden)\n",
    "        \n",
    "        # Réinitialiser le gradient.\n",
    "        optimizer.zero_grad()\n",
    "                \n",
    "        # Forward pass.\n",
    "        y_pred, hidden = model(x, hidden)\n",
    "        \n",
    "        # Calculer la perte.\n",
    "        loss = loss_fun(y_pred.view(-1, vocab_size), y.view(-1))\n",
    "        \n",
    "        # Calculer le gradient.\n",
    "        loss.backward()\n",
    "        \n",
    "        # Normaliser le gradient pour éviter explosion.\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
    "        \n",
    "        # Mettre à jour les paramètres du modèle.\n",
    "        optimizer.step()        \n",
    "        \n",
    "        # Accumuler la perte total.\n",
    "        train_loss += len(x) * loss.item()\n",
    "    \n",
    "    # Évaluer le modèle sur l'ensemble de validation.\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(batch_size)\n",
    "        for x, y in valid_loader:\n",
    "            y_pred, hidden = model(x, hidden)\n",
    "            loss = loss_fun(y_pred.view(-1, vocab_size), y.view(-1))\n",
    "            valid_loss += len(x) * loss.item()\n",
    "    \n",
    "    train_loss /= len(train_loader.dataset)\n",
    "    valid_loss /= len(valid_loader.dataset)\n",
    "    scheduler.step(valid_loss)\n",
    "\n",
    "    print(\"Epoch {:2d} | Training loss = {:.5f} | Validation loss = {:.5f} | Perplexity = {:.2f}\"\n",
    "          .format(epoch+1, train_loss, valid_loss, np.exp(valid_loss)))\n",
    "print(\"Félicitations! Vous avez terminé d'entraîner votre beau modèle de langue neuronal!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tKAOoUwWV8To"
   },
   "source": [
    "## Génération de texte\n",
    "\n",
    "Maintenant que nous avons entraîné un modèle linguistique, nous pouvons l'utiliser pour générer un texte comme Shakespeare ! \n",
    "Pour ce faire, nous sélectionnerons au hasard le premier mot (c'est-à-dire un jeton dans un vocabulaire) et l'utiliserons comme jeton d'entrée à la première étape. Nous utiliserons ensuite le mot de sortie au premier pas de temps comme mot d'entrée au deuxième pas de temps et ainsi de suite. Au total, nous allons générer des `n_words`.La variable `smoothing` permet d'obtenir une modification de la diversité du texte généré. Une valeur plus élevée permet de générer un texte plus diversifié mais souvent de moindre qualité."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lLg-dbOSsuVu"
   },
   "outputs": [],
   "source": [
    "# Génération de séquences de mots.\n",
    "model.eval()\n",
    "x = torch.randint(0, vocab_size, (1, 1), dtype=torch.long, device=DEVICE)\n",
    "words = [rev_vocab[x]]\n",
    "n_words = 300\n",
    "smoothing = 0.7\n",
    "with torch.no_grad(): \n",
    "    hidden = model.init_hidden(1)\n",
    "    for i in range(n_words-1):\n",
    "        output, hidden = model(x, hidden)\n",
    "        weights = output.squeeze().div(smoothing).exp()\n",
    "        word_idx = torch.multinomial(weights, 1)\n",
    "        x.fill_(word_idx.squeeze())\n",
    "        word = rev_vocab[word_idx]\n",
    "        words.append(word)\n",
    "        if (i+1) % 15 == 0:\n",
    "            words.append(\"\\n\")\n",
    "print(\" \".join(words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8HI-Elp2UhBY"
   },
   "source": [
    "Qu'en pensez-vous? À première vue, bien que certaines séquences de mots peuvent être cocasses, la clarté du texte généré n'est pas la meilleure. En effet, nous observons que la virgule (`,`) apparaît souvent dans le texte et cela est dû au fait que la virgule s'agit du token le plus fréquent dans notre corpus (rappelez-vous que les 10 tokens les plus fréquents sont `[',', 'de', '.', 'et', 'la', 'le', 'les', ';', \"l'\", 'il']`.  Pour améliorer la clarté du texte généré, il serait nécessaire d'entraîner le modèle sur un corpus de plus grande taille. De plus, nous soulignons que le vocabulaire et le style littéraire de notre cher Voltaire ne sont pas les plus conventionnels! Au final, il ne faut pas oublier que nous avons simplement appris une distribution de probabilité basée sur les statistiques d'un corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gUtkYczw4Pg-"
   },
   "source": [
    "---\n",
    "## Références\n",
    "* http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\n",
    "* https://arxiv.org/abs/1803.08240\n",
    "\n",
    "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "RNN_a_completer.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
