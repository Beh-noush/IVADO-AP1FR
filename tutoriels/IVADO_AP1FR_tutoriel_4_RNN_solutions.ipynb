{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "RNN_solutions.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B9ALu78bhPXM"
      },
      "source": [
        "# ÉCOLE IVADO/Mila EN APPRENTISSAGE PROFOND\n",
        "# SESSION D'AUTOMNE 2018 \n",
        "# Tutoriel : Réseaux de neurones récurrents (RNR/RNN)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MIH_PfZV1rNa"
      },
      "source": [
        "## Auteurs\n",
        "\n",
        "Francis Grégoire <francis.gregoire@rd.mila.quebec>\n",
        "\n",
        "Jeremy Pinto <jeremy.pinto@rd.mila.quebec>\n",
        "\n",
        "Jean-Philippe Reid <Jean-Philippe.Reid@ElementAI.com>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OYrPFO1p1rX1"
      },
      "source": [
        "## Préface\n",
        "\n",
        "Ce tutoriel contient deux exemples ayant comme objectif d'introduire des concepts fondamentaux sur les réseaux de neurones récurrents (RNR/RNN et LMCT/LSTM).\n",
        "\n",
        "La première tâche consiste d'un exemple plutôt élémentaire servant à mettre en évidence l'avantage d'utiliser un modèle LSTM contre un simple RNN.\n",
        "\n",
        "Dans la deuxième tâche, nous allons mettre en valeur un modèle LSTM de manière plus concrète en développant un modèle de langue neuronal pour générer du nouveau texte. Dans cet exemple, vous allez apprendre comment faire le prétraitement des données textuelles dans le but de réussir à entraîner un modèle de langue neuronal de manière efficace."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ArGnixElhPXN"
      },
      "source": [
        "---\n",
        "# Initialisation et importation des librairies\n",
        "\n",
        "Pour assurer le bon fonctionnement de ce notebook sur Colab, il est nécessaire d'installer quelques librairies à l'aide de `pip`. Tout d'abord, assurez-vous d'être connectés au notebook (✓ CONNECTED en haut à droite). Exécutez ensuite la cellule suivante en la sélectionnant et en cliquant `shift`+`Enter`. L'installation peut prendre quelques minutes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "028-EhOGhPXO",
        "outputId": "99f1d732-f8c9-4910-bcfa-2abe8e1b5a88"
      },
      "source": [
        "!pip install torch torchvision matplotlib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /home/andrew/anaconda3/lib/python3.7/site-packages (1.6.0)\n",
            "Requirement already satisfied: torchvision in /home/andrew/anaconda3/lib/python3.7/site-packages (0.7.0)\n",
            "Requirement already satisfied: matplotlib in /home/andrew/anaconda3/lib/python3.7/site-packages (3.1.3)\n",
            "Requirement already satisfied: numpy in /home/andrew/anaconda3/lib/python3.7/site-packages (from torch) (1.18.1)\n",
            "Requirement already satisfied: future in /home/andrew/anaconda3/lib/python3.7/site-packages (from torch) (0.18.2)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /home/andrew/anaconda3/lib/python3.7/site-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /home/andrew/anaconda3/lib/python3.7/site-packages (from matplotlib) (1.1.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /home/andrew/anaconda3/lib/python3.7/site-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /home/andrew/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /home/andrew/anaconda3/lib/python3.7/site-packages (from matplotlib) (2.4.6)\n",
            "Requirement already satisfied: setuptools in /home/andrew/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib) (45.2.0.post20200210)\n",
            "Requirement already satisfied: six in /home/andrew/anaconda3/lib/python3.7/site-packages (from cycler>=0.10->matplotlib) (1.14.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwebZdYMhPXT",
        "outputId": "d2079fd1-f5c1-4775-bdf0-73a21f733b1f"
      },
      "source": [
        "import copy\n",
        "import time\n",
        "import os\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.nn.init as init\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "\n",
        "use_gpu = torch.cuda.is_available()\n",
        "device = torch.device(\"cuda:0\" if use_gpu else \"cpu\")\n",
        "\n",
        "# Graine pour reproduire les résultats.\n",
        "seed = 42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "\n",
        "print(\"version PyTorch: \", torch.__version__)\n",
        "print(\"GPU disponible: {}\".format(use_gpu))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "version PyTorch:  1.6.0\n",
            "GPU disponible: False\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7SLcPxj5z2vX"
      },
      "source": [
        "---\n",
        "# Tâche 1: Sommes de nombres"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4Y-PnqdMhPXX"
      },
      "source": [
        "\n",
        "## Objectif\n",
        "\n",
        "L'objectif de cet exemple est de construire un modèle capable de faire la somme d'une série de nombres. Il s'agit d'une tâche très simple (une calculatrice de poche peut facilement l'exécuter!) et servira à souligner certaines limitations des RNN. L'ensemble de données est facile à générer et nous permet de rapidement tester la capacité à modéliser une longue séquence entre un RNN traditionnel et un LSTM.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nihkB--rz6WL"
      },
      "source": [
        "## Ensemble de données\n",
        "\n",
        "L'ensemble de données est constitué d'un ensemble de séquences de nombres de longueur $seq\\_len$ où une cible est associée à chacune. Dans notre cas, la cible d'une séquence est associée à la somme des nombres de la séquence. Ainsi, pour un exemple donné nous avons comme variable d'entrée un vecteur $\\mathbf x^{(i)} = \\left[x_{1}^{i}, x_{2}^{i}, \\dots, x_{T}^{N}\\right]$ de longueur $seq\\_len=T$ et comme cible la variable $y^{(i)}$ donnée par:\n",
        "\n",
        "\\begin{align}  \n",
        "y^{(i)}=\\sum_{j=1}^{seq\\_len}x^{(i)}_j.\n",
        "\\end{align}\n",
        "\n",
        "Où $j$ est l'indice de temps.\n",
        "\n",
        "Par exemple, pour un $\\mathbf x^{(i)}$ explicitement défini avec $seq\\_len=4$, nous avons:\n",
        "\n",
        "\n",
        "\\begin{align}  \n",
        "\\mathbf x^{(i)} &= \\left[ 4,-1,15,24\\right], \\, \\mathbf x^{(i)} \\in \\mathbb R^{4}; \\\\ \n",
        "y^{(i)} &= 42, \\, \\mathrm y^{(i)} \\in \\mathbb R.\n",
        "\\end{align}\n",
        "\n",
        "Nous allons nous servir de cet ensemble de données pour entrainer un RNN et un LSTM. Comme la cible est un nombre réel, nous devons utiliser une couche linéaire pour projeter le dernier état récurrent du RNN/LSTM, $h^{(i)}_{T}$, tel qu'illustré à la figure suivante:\n",
        "\n",
        "![Texte alternatif…](https://github.com/jphreid/tutorial_ivado/raw/master/lstm-figures.002.jpeg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zZNrVvS8hPXY"
      },
      "source": [
        "### Génération d'un ensemble de données\n",
        "\n",
        "Afin de mieux énoncer la tâche, nous définissons une fonction utilitaire qui nous permet de construire un ensemble de données aléatoires de `n_samples` séquences de longueur `seq_len` en utilisant la fonction [torch.randint()](https://pytorch.org/docs/stable/torch.html#torch.randint).La fonction `generate_data` prend en entrée les arguments suivants :\n",
        "- **n_samples** (int) : nombre de séquences à générer.\n",
        "- **seq_len** (int) : longueur de chaque séquence.\n",
        "- **input_dim** (int, optionnel) : dimension des données d'entrée. Valeur par défaut : 1.\n",
        "- **xmin** (float, optionnel) : valeur minimale possible dans la séquence. Valeur par défaut : -100.\n",
        "- **xmax** (float, optionnel) : valeur maximale possible dans la séquence. Valeur par défaut : -100.\n",
        "\n",
        "Elle renvoie un tuple de deux éléments correspondant respectivement à :\n",
        "- **X** ([torch.FloatTensor](https://pytorch.org/docs/stable/tensors.html)) : Un tenseur de forme $n\\_samples \\times seq\\_len \\times input\\_dim$ représentant un ensemble de séquences `n_samples`, chacune de longueur `seq_len`. Les éléments des séquences sont de dimension `input_dim`, c'est-à-dire qu'ils appartiennent à ${\\mathbb R}^{input\\_dim}$.\n",
        "- **Y** ([torche.FloatTensor](https://pytorch.org/docs/stable/tensors.html)) : Un tenseur de forme $n\\_samples \\times input\\_dim$ représentant la somme correspondante des éléments pour chaque séquence dans `X`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "nKFfyWFahPXZ"
      },
      "source": [
        "def generate_data(n_samples, seq_len, input_dim, xmin=-100, xmax=100):\n",
        "    \"\"\"Générer des tenseurs X et Y dans l'intervalle [xmin, xmax].\n",
        "    \n",
        "    Args : \n",
        "      n_samples: int, nombre de séquences à générer.\n",
        "      seq_len: int, longueur de chaque séquence.\n",
        "      xmin: valeur minimale que peut prendre n'importe quel nombre d'une séquence.\n",
        "      xmax: valeur maximale que peut prendre n'imoprte quel nombre d'une séquence.\n",
        "    \n",
        "    Returns: retourne les séquences de nombres X et les cibles Y en format \n",
        "             torch.Tensor où X.shape = (n_samples, seq_len, 1) et\n",
        "             Y.shape = (n_samples, 1).\n",
        "    \"\"\"\n",
        "    \n",
        "    X = torch.randint(xmin, xmax+1, (n_samples, seq_len, input_dim))\n",
        "    Y = X.sum(dim=1)\n",
        "    \n",
        "    return X, Y"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-uhue_rShPXc"
      },
      "source": [
        "Le code suivant montre un exemple d'utilisation de la fonction `generate_data`. Dans cet exemple, nous générons 1000 séquences de nombres $\\in \\mathbb R$ (c'est-à-dire que `input_dim` est fixé à 1), chacune de longueur 4."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "xSpCvlgik55y",
        "outputId": "c468c179-241f-4782-f446-806af6764b45"
      },
      "source": [
        "n_samples = 1000\n",
        "seq_len = 4\n",
        "input_dim = 1\n",
        "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
        "print(\"Dimensions du tenseur X = {}\".format(X.shape))\n",
        "print(\"où n_samples = {}, seq_len = {}, input_dim = {}\".format(*X.shape))\n",
        "print(\"exemple de donnée: {}\".format(X[0,:,0]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensions du tenseur X = torch.Size([1000, 4, 1])\n",
            "où n_samples = 1000, seq_len = 4, input_dim = 1\n",
            "exemple de donnée: tensor([ 71,  40, -84,  81])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1wHZ_PFhPXf"
      },
      "source": [
        "### Standardisation des données\n",
        "\n",
        "Afin d'aider notre modèle lors de l'entraînement, nous standardisons nos données de sorte qu'elles aient une moyenne de 0 et un écart type de 1. Afin de pouvoir retransformer nos données, nous devons conserver les valeurs de la moyenne et de l'écart type employées. \n",
        "\n",
        "Note: étant donné que nous échantillonnons à partir d'un ensemble uniformément distribué, l'écart type devrait approcher $\\frac{(xmax-xmin)}{\\sqrt{12}}$ et la moyenne $\\frac{(xmax-xmin)}{2}$.\n",
        "\n",
        "La fonction suivante effectue une telle opération de normalisation. Elle prend en entrée un tenseur **X** (de forme $n\\_samples \\times seq\\_len \\times input\\_dim$) que nous souhaitons normaliser et retourne un tuple de 4 éléments correspondant respectivement à :\n",
        "- **Xs** : la version standardisée de X, de forme $n\\_samples \\times seq\\_len \\times input\\_dim$\n",
        "- **Ys** : la nouvelle somme des séquences de Xs, de forme $n\\_samples \\times input\\_dim$\n",
        "- **moyenne** : la moyenne de X, float.\n",
        "- **stdev** : l'écart type de X, float."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UdL_y85mhPXg"
      },
      "source": [
        "def standardize(X):\n",
        "    \"\"\"Cette fonction standardise le tenseur X. \n",
        "    Args:\n",
        "      X: torch.Tensor.\n",
        "    \n",
        "    Returns:\n",
        "      X: torch.Tensor standardisé.\n",
        "      Y: torch.Tensor, somme du tenseur X.\n",
        "      mean: torch.mean, moyenne du tenseur X\n",
        "      std: torch.std, écart type de X\n",
        "    \"\"\"\n",
        "    \n",
        "    X=X.float()\n",
        "    mean = torch.mean(X)\n",
        "    std = torch.std(X)\n",
        "    Xs = (X-mean) / std\n",
        "    Ys = Xs.sum(dim=1)\n",
        "    \n",
        "    return Xs, Ys, mean, std"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B-ij_r5Gk557"
      },
      "source": [
        "Un exemple d'utilisation de la fonction définie ci-dessus est présenté ci-dessous. Ici, après avoir généré les données en utilisant la fonction `generate_data`, nous utilisons la fonction `standardize` pour les standardiser."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LlH5mMctk558",
        "outputId": "54900fe3-57d5-47a0-9cc6-9f192ccb6384"
      },
      "source": [
        "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
        "example_before = X[0,:,0]\n",
        "Xs, Ys, mean, std = standardize(X)\n",
        "print(\"moyenne = {:.4f}, écart-type = {:.4f}\".format(mean, std))\n",
        "print('exemple avant standardization: {}'.format(X[0,:,0]),\n",
        "      '\\nexemple après standardization: {}'.format(Xs[0,:,0])\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "moyenne = 0.0975, écart-type = 57.8716\n",
            "exemple avant standardization: tensor([ 15, -21,  65,  90]) \n",
            "exemple après standardization: tensor([ 0.2575, -0.3646,  1.1215,  1.5535])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCm8xalthPXj"
      },
      "source": [
        "## Implémentation d'un RNN\n",
        "\n",
        "Nous allons définir notre RNN en utilisant la classe [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn). Pour plus de détails sur l'implémentation de cette classe, vous pouvez consulter ce [tutoriel](https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#example-2-recurrent-net). Une fois initialisée, cette classe accepte comme données d'entrées `X` de dimensions `(seq_len, batch_size, input_size)` (avec `input_size=1` dans notre exemple). Nous devons ajouter une couche linéaire pour transformer la prédiction en sortie du RNN dans la même dimension que notre cible `Y` de dimensions `(batch_size, output_size)` (avec `output_size = input_size = 1` dans notre exemple). \n",
        "\n",
        "Pour définir l'architecture de notre RNN, nous utiliserons le module [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn) suivi d'une couche linéaire [torch.nn.Linear()](https://pytorch.org/docs/stable/nn.html#linear). Les méthodes suivantes sont à compléter:\n",
        "<ul>\n",
        "<li>La méthode `__init__()` qui définit les couches du modèle. </li>\n",
        "<li>La méthode `forward()` qui utilise les couches et des variables en entrée pour retourner une sortie (équivalent à une propagation avant).</li>\n",
        "</ul>\n",
        "**Notes importantes**: \n",
        "\n",
        "* * Rappelez-vous que seule la dernière sortie du RNN est nécessaire, c'est-à-dire, `output[-1]`.\n",
        "\n",
        "* Vous devez vous assurer que les dimensions de `X` en entrée soient conséquents avec l'implémentation de [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn). \n",
        "    \n",
        "    **Indice:** La fonction [tensor.transpose()](https://pytorch.org/docs/stable/tensors.html?highlight=transpose#torch.Tensor.transpose) peut vous être utile."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B29-Mgxuk56B"
      },
      "source": [
        "### Exercice :\n",
        "\n",
        "Complétez ce morceau de code pour implémenter le réseau décrit ci-dessus en utilisant la classe [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn). Les arguments d'entrée sont les suivants :\n",
        "- **input_dim** : la dimension des données d'entrée\n",
        "- **output_dim** : la dimension des données de sortie\n",
        "- **hiden_size** : la taille de l'état caché du RNN\n",
        "- **n_couches** : le nombre de couches du RNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QCVSZGzhPXk"
      },
      "source": [
        "class RNNLinear(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, output_dim, hidden_size, n_layers):\n",
        "        super(RNNLinear, self).__init__()\n",
        "        self.rnn = nn.RNN(input_dim,\n",
        "                          hidden_size,\n",
        "                          n_layers)\n",
        "        \n",
        "        self.linear = nn.Linear(hidden_size, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # l'entrée d'un RNN doit être de dimensions (seq_len, batch_size, input_size)\n",
        "        x = x.transpose(0, 1) \n",
        "        output, _ = self.rnn(x)\n",
        "        pred = self.linear(output[-1])\n",
        "        return pred\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQuuGdgtk56G"
      },
      "source": [
        "Voici un exemple de la façon dont cette classe peut être utilisée pour prédire les valeurs des séquences en X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gdXvsqP1k56H",
        "outputId": "51099401-b3ce-401d-83f2-99fbd4dac52c"
      },
      "source": [
        "n_samples = 50\n",
        "seq_len = 4\n",
        "input_dim = 1\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "hidden_size = 20\n",
        "\n",
        "# Génération des données\n",
        "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
        "Xs, Ys, mean, std = standardize(X)\n",
        "\n",
        "# Déclarer le modèle RNN.\n",
        "model_rnn = RNNLinear(input_dim, output_dim, hidden_size, n_layers).to(device)\n",
        "\n",
        "# Transférer le modèle au bon matériel\n",
        "model_rnn = model_rnn.to(device)\n",
        "\n",
        "# Sauvegarder les poids initiaux du modèle.\n",
        "init_rnn_weights = copy.deepcopy(model_rnn.state_dict())\n",
        "\n",
        "# Transférer les données au bon matériel\n",
        "Xs = Xs.to(device)\n",
        "\n",
        "# Utiliser le RNN pour faire la prédiction de tous les exemples sans entraînement (c.à.d. sur les n_samples)\n",
        "# Vérifiez que les inputs d'entrée et sorties sont justes\n",
        "y_pred = model_rnn(Xs)\n",
        "print(\"Dimensions initiales des données en entrée: {}\".format(X.shape)) # (seq_len, batch_size, input_size)\n",
        "print(\"Dimensions des prédictions: {}\".format(y_pred.shape)) # (batch_size, input_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensions initiales des données en entrée: torch.Size([50, 4, 1])\n",
            "Dimensions des prédictions: torch.Size([50, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg8vsRduhPYH"
      },
      "source": [
        "## Implémentation d'un LSTM\n",
        "\n",
        "Nous allons maintenant implémenter un LSTM. Nous utilisons la classe [torch.nn.LSTM()](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM). Comme avec le RNN, nous devons ajouter une couche linéaire pour transformer la prédiction en sortie du LSTM dans la même dimension que notre cible `Y` de dimensions `(batch_size, output_size)` (avec `output_size = input_size = 1` dans notre exemple). Pour plus de détails sur l'implémentation de cette classe, consultez ce [tutoriel](https://pytorch.org/tutorials/beginner/former_torchies/nn_tutorial.html#example-2-recurrent-net).\n",
        "\n",
        "Pour définir l'architecture de notre LSTM, les méthodes suivantes sont à compléter:\n",
        "<ul>\n",
        "<li>La méthode `__init__()` qui définit les couches du modèle. </li>\n",
        "<li>La méthode `forward()` qui utilise les couches et des variables en entrée pour retourner une sortie (équivalent à une propagation avant).</li>\n",
        "</ul>\n",
        "\n",
        "**Notes importantes**: \n",
        "\n",
        "*  il peut être déroutant d'obtenir le dernier état récurrent de la dernière couche cachée d'un LSTM, $h_{T}^{N}$, où $T$ est le dernier pas de temps et $N$ est la dernière couche cachée. Dans le cas d'un LSTM, le dernier état récurrent de la dernière couche cachée peut être obtenu en indexant ces tenseurs de cette manière: `output[-1, :, :]` ou `hidden_T[0][-1]`.\n",
        "\n",
        "* Vous devez vous assurer que les dimensions de `X` en entrée soient conséquents avec l'implémentation de [torch.nn.RNN()](https://pytorch.org/docs/stable/nn.html?highlight=rnn#rnn)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JWymVwgCk56L"
      },
      "source": [
        "### Exercice :\n",
        "\n",
        "Complétez ce morceau de code pour implémenter le réseau décrit ci-dessus en utilisant la classe [torch.nn.LSTM()](https://pytorch.org/docs/stable/nn.html?highlight=lstm#torch.nn.LSTM). Les arguments d'entrée sont les suivants :\n",
        "- **input_dim** : la dimension des données d'entrée\n",
        "- **output_dim** : la dimension des données de sortie\n",
        "- **hiden_size** : la taille de l'état caché du LSTM\n",
        "- **n_couches** : le nombre de couches du LSTM."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lqMaUDDVhPYH"
      },
      "source": [
        "class LSTMLinear(nn.Module):\n",
        "    \n",
        "    def __init__(self, input_dim, output_dim, hidden_size, n_layers):\n",
        "        super(LSTMLinear, self).__init__()\n",
        "        self.LSTM = nn.LSTM(input_dim,\n",
        "                            hidden_size,\n",
        "                            n_layers)\n",
        "        \n",
        "        self.linear = nn.Linear(hidden_size, output_dim)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        # l'input d'un LSTM doit être de dimensions (seq_len, batch_size, input_size)\n",
        "        x = x.transpose(0, 1)\n",
        "        output, _ = self.LSTM(x)\n",
        "        pred = self.linear(output[-1])\n",
        "        \n",
        "        return pred\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cv3k9ehzk56R"
      },
      "source": [
        "Voici un exemple de la façon dont cette classe peut être utilisée pour prédire les valeurs des séquences en X."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YtZmQR2k56S",
        "outputId": "19ff5053-9013-4493-aeb0-c10437d2b29b"
      },
      "source": [
        "n_samples = 50\n",
        "seq_len = 4\n",
        "input_dim = 1\n",
        "output_dim = 1\n",
        "n_layers = 2\n",
        "hidden_size = 20\n",
        "\n",
        "# Génération des données\n",
        "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
        "Xs, Ys, mean, std = standardize(X)\n",
        "\n",
        "# Déclarer le modèle LSTM.\n",
        "model_lstm = LSTMLinear(input_dim, output_dim, hidden_size, n_layers).to(device)\n",
        "\n",
        "# Transférer le modèle au bon matériel\n",
        "model_lstm = model_lstm.to(device)\n",
        "\n",
        "# Sauvegarder les poids initiaux du modèle.\n",
        "init_lstm_weights = copy.deepcopy(model_lstm.state_dict())\n",
        "\n",
        "# Transférer les données au bon matériel\n",
        "Xs = Xs.to(device)\n",
        "\n",
        "# Utiliser le LSTM pour faire la prédiction de tous les exemples sans entraînement(c.à.d. sur les n_samples)\n",
        "# Vérifiez que les inputs d'entrée et sorties sont justes\n",
        "y_pred = model_lstm(Xs)\n",
        "print(\"Dimensions initiales des données en entrée: {}\".format(X.shape)) # (seq_len, batch_size, input_size)\n",
        "print(\"Dimensions des prédictions: {}\".format(y_pred.shape)) # (batch_size, input_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dimensions initiales des données en entrée: torch.Size([50, 4, 1])\n",
            "Dimensions des prédictions: torch.Size([50, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gdha8WWWhPXp"
      },
      "source": [
        "## Préparation des données\n",
        "\n",
        "### Exercice\n",
        "\n",
        "Nous allons préparer nos objets DataLoader pour gérer efficacement les ensembles de données que nous allons générer. \n",
        "\n",
        "Pour cet exemple, nous utilisons 20,000 séquences où 80% des données servent comme ensemble d'entraînement, 10% comme ensemble de validation et 10% comme ensemble d'évaluation. Nous pouvons utiliser les fonctions [torch.utils.data.TensorDataset()](https://pytorch.org/docs/stable/data.html) et [torch.utils.data.DataLoader()](https://pytorch.org/docs/stable/data.html) afin de préparer les Dataloader.\n",
        "\n",
        "Utilisez les valeurs suivantes:\n",
        "\n",
        "`seq_len = 18` \n",
        "\n",
        "`batch_size = 64`\n",
        "\n",
        "`n_samples = 25000`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "code_folding": [],
        "id": "PzSCKyyMhPXq"
      },
      "source": [
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "n_samples = 25000\n",
        "seq_len = 18\n",
        "batch_size = 64\n",
        "\n",
        "# Génération des données:\n",
        "X, Y = generate_data(n_samples, seq_len, input_dim, -100, 100)\n",
        "\n",
        "# et les standardiser:\n",
        "Xs, Ys, mean, std = standardize(X)\n",
        "\n",
        "xtrain, ytrain = Xs[:round(0.8*n_samples)], Ys[:round(0.8*n_samples)]\n",
        "xvalid, yvalid = Xs[round(0.8*n_samples):round(0.9*n_samples)], Ys[round(0.8*n_samples):round(0.9*n_samples)]\n",
        "xtest, ytest = Xs[round(0.9*n_samples):], Ys[round(0.9*n_samples):]\n",
        "\n",
        "# dataloader pour l'ensemble d'entraînement\n",
        "train_loader = DataLoader(TensorDataset(xtrain, ytrain), batch_size, shuffle=True)\n",
        "\n",
        "# dataloader pour l'ensemble de validation\n",
        "valid_loader = DataLoader(TensorDataset(xvalid, yvalid), batch_size, shuffle=False)\n",
        "\n",
        "# dataloader pour l'ensemble d'évaluation\n",
        "test_loader = DataLoader(TensorDataset(xtest, ytest), batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8IdyMkZjRs7"
      },
      "source": [
        "## Entraînement du RNN\n",
        "\n",
        "De nombreuses fonctions de coût et optimiseurs sont disponibles dans PyTorch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-KSQ72HhPXt"
      },
      "source": [
        "### Exercice: Définir la fonction de coût et l'optimiseur\n",
        "\n",
        "Rappelons qu'une fonction de coût $J(\\theta) = L(x, y, \\theta)$ prend en entrée le tuple (prédiction, cible) et calcule une valeur qui estime la distance entre la prédiction et la cible. L'optimiseur que nous utilisons dans cet exemple est celui de la descente de gradient stochastique (*stochastic gradient descent* (SGD)). Il minimise la fonction de coût $J(\\theta)$ paramétrisée par les poids du modèle $\\theta$ en mettant à jour les poids itérativement suivant la règle de mise à jour suivante: $\\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta)$, où  $\\alpha$ est le taux d'apprentissage (*learning rate*).\n",
        "\n",
        "Pour un problème de régression comme nous avons dans cet exemple, un choix commun est d'utiliser les fonctions suivantes :\n",
        "<ul>\n",
        "<li>**Fonction de coût :** <a href=\"https://pytorch.org/docs/stable/nn.html\">`torch.nn.MSELoss()`</a>. L'erreur quadratique moyenne permet de calculer la moyenne de l'écart au carré entre la valeur prédite et la valeur désirée. Cette fonction est définie comme:\n",
        "\n",
        "$J(\\cdot) = \\frac{1}{N}\\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^{2}$.\n",
        "    \n",
        "<li>**Optimiseur :** <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD()`</a> qui est une implémentation de Stochastic Gradient Descent.</li>\n",
        "</ul>\n",
        "\n",
        "Nous utilisons une valeur de taux d'apprentissage de 0.01."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6SidKvG6hPXu"
      },
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "# Définir le critère\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Définir l'optimiseur\n",
        "optimizer_rnn = optim.SGD(model_rnn.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pBrLb8PdhPXw"
      },
      "source": [
        "### Exercice: Entraînement du modèle\n",
        "\n",
        "Nous utilisons notre objet `train_loader` pour itérer à travers notre ensemble d'entraînement *n_epoch* fois pour faire l'entraînement du modèle. La valeur accumulée de la fonction de coût évaluée sur l'ensemble de validation est sauvegardée à la fin de chaque *epoch* d'entraînement.Nous utiliserons `n_epoch = 25`.\n",
        "\n",
        "Complétez le code suivant avec les instructions correspondant aux commentaires correspondants."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9a-gjjVhPXx",
        "outputId": "3ff63061-9b7f-48b3-f9c4-32e8502d00f5"
      },
      "source": [
        "since = time.time()\n",
        "\n",
        "\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "n_epoch = 25\n",
        "\n",
        "model_rnn.load_state_dict(init_rnn_weights)\n",
        "\n",
        "print(\"Début de l'entraînement\")\n",
        "\n",
        "for epoch in range(n_epoch):\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_n_iter = 0\n",
        "    \n",
        "    # Mettre le modèle en mode d'entraînement\n",
        "    model_rnn.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    \n",
        "    # Itérer sur les données d'entraînement\n",
        "    for x, y in train_loader:  \n",
        "\n",
        "        \n",
        "        # Mettre les tenseurs sur le matériel (GPU si possible)\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Réinitisaliser les gradients à zéro\n",
        "        optimizer_rnn.zero_grad()\n",
        "        \n",
        "        # Exécuter la propagation avant\n",
        "        outputs = model_rnn(x)\n",
        "        \n",
        "        # Calculer la perte avec le critère\n",
        "        loss = criterion(outputs, y)\n",
        "        \n",
        "        # Exécuter la propagation arrière\n",
        "        loss.backward()\n",
        "        \n",
        "        # Exécuter le pas d'optimisation\n",
        "        optimizer_rnn.step()\n",
        "        \n",
        "        # Statistiques\n",
        "        train_loss += loss.item()\n",
        "        train_n_iter += 1\n",
        "    \n",
        "    valid_loss = 0\n",
        "    valid_n_iter = 0\n",
        "    \n",
        "    # Mettre le modèle en mode d'évaluation\n",
        "    model_rnn.eval()\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        # Itérer sur les données de validation\n",
        "        for x, y in valid_loader:  \n",
        "        \n",
        "            # Mettre les tenseurs sur le matériel (GPU si possible)\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "        \n",
        "            # Exécuter la propagation avant\n",
        "            outputs = model_rnn(x)\n",
        "            \n",
        "            # Calculer la perte avec le critère\n",
        "            loss = criterion(outputs, y)\n",
        "        \n",
        "            # Statistiques\n",
        "            valid_loss += loss.item()\n",
        "            valid_n_iter += 1\n",
        "    \n",
        "    train_loss_history.append(train_loss / train_n_iter)\n",
        "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
        "\n",
        "\n",
        "    print(\"Époque {:2d} | Perte d'entraînement = {:.5f} | Perte de validation = {:.5f} \"\n",
        "          .format(epoch+1, (train_loss / train_n_iter), (valid_loss / valid_n_iter)))\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "\n",
        "print('\\n\\nEntraînement fini en {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Début de l'entraînement\n",
            "Époque  1 | Perte d'entraînement = 11.92072 | Perte de validation = 4.67765 \n",
            "Époque  2 | Perte d'entraînement = 4.62592 | Perte de validation = 2.67881 \n",
            "Époque  3 | Perte d'entraînement = 3.67357 | Perte de validation = 3.62893 \n",
            "Époque  4 | Perte d'entraînement = 2.80312 | Perte de validation = 1.59725 \n",
            "Époque  5 | Perte d'entraînement = 2.09027 | Perte de validation = 3.77149 \n",
            "Époque  6 | Perte d'entraînement = 1.79430 | Perte de validation = 1.72656 \n",
            "Époque  7 | Perte d'entraînement = 1.64189 | Perte de validation = 0.77107 \n",
            "Époque  8 | Perte d'entraînement = 1.46530 | Perte de validation = 2.06042 \n",
            "Époque  9 | Perte d'entraînement = 1.34344 | Perte de validation = 1.48019 \n",
            "Époque 10 | Perte d'entraînement = 1.12551 | Perte de validation = 0.52266 \n",
            "Époque 11 | Perte d'entraînement = 1.01496 | Perte de validation = 0.94068 \n",
            "Époque 12 | Perte d'entraînement = 0.99228 | Perte de validation = 0.66603 \n",
            "Époque 13 | Perte d'entraînement = 0.94427 | Perte de validation = 0.44831 \n",
            "Époque 14 | Perte d'entraînement = 0.78657 | Perte de validation = 0.52230 \n",
            "Époque 15 | Perte d'entraînement = 0.81133 | Perte de validation = 0.64304 \n",
            "Époque 16 | Perte d'entraînement = 0.71797 | Perte de validation = 0.95602 \n",
            "Époque 17 | Perte d'entraînement = 0.72770 | Perte de validation = 0.80690 \n",
            "Époque 18 | Perte d'entraînement = 0.59290 | Perte de validation = 0.59735 \n",
            "Époque 19 | Perte d'entraînement = 0.66520 | Perte de validation = 0.38693 \n",
            "Époque 20 | Perte d'entraînement = 0.55255 | Perte de validation = 1.06397 \n",
            "Époque 21 | Perte d'entraînement = 0.50574 | Perte de validation = 0.77140 \n",
            "Époque 22 | Perte d'entraînement = 0.54244 | Perte de validation = 0.66025 \n",
            "Époque 23 | Perte d'entraînement = 0.51906 | Perte de validation = 0.40375 \n",
            "Époque 24 | Perte d'entraînement = 0.48106 | Perte de validation = 0.59619 \n",
            "Époque 25 | Perte d'entraînement = 0.41404 | Perte de validation = 0.44855 \n",
            "\n",
            "\n",
            "Entraînement fini en 0m 50s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wr4at5nVhPXz"
      },
      "source": [
        "### Visualisation des courbes d'entraînement\n",
        "\n",
        "Visualisez les courbes d'entraînement avec un graphique mettant en valeur la fonction de coût vs. le nombre d'époques pour les ensembles d'entraînement et de validation sur un même graphe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJLu-rC_hPX1",
        "outputId": "37daabe8-da48-439b-b9c9-1a7ae517cb18"
      },
      "source": [
        "# Enregistrer l'historique\n",
        "rnn_train_loss_history = train_loss_history\n",
        "rnn_valid_loss_history = valid_loss_history\n",
        "\n",
        "# Tracer les courbes d'entraînement et de validation\n",
        "xaxis = range(1, n_epoch + 1)\n",
        "plt.plot(xaxis, rnn_train_loss_history, label='entraînement-rnn')\n",
        "plt.plot(xaxis, rnn_valid_loss_history, label='validation-rnn')\n",
        "\n",
        "plt.xlabel('# époques')\n",
        "plt.ylabel('Perte')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vi7bHNzykdby"
      },
      "source": [
        "## Entraînement LSTM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QoahNuS73snl"
      },
      "source": [
        "### Exercice: Définir la fonction de coût et l'optimiseur\n",
        "\n",
        "Rappelons qu'une fonction de coût $J(\\theta) = L(x, y, \\theta)$ prend en entrée le tuple (prédiction, cible) et calcule une valeur qui estime la distance entre la prédiction et la cible. L'optimiseur que nous utilisons dans cet exemple est celui de la descente de gradient stochastique (*stochastic gradient descent* (SGD)). Il minimise la fonction de coût $J(\\theta)$ paramétrisée par les poids du modèle $\\theta$ en mettant à jour les poids itérativement suivant la règle de mise à jour suivante: $\\theta \\leftarrow \\theta - \\alpha \\nabla J(\\theta)$, où  $\\alpha$ est le taux d'apprentissage (*learning rate*).\n",
        "\n",
        "Pour un problème de régression comme nous avons dans cet exemple, un choix commun est d'utiliser les fonctions suivantes :\n",
        "<ul>\n",
        "<li>**Fonction de coût :** <a href=\"https://pytorch.org/docs/stable/nn.html\">`torch.nn.MSELoss()`</a>. L'erreur quadratique moyenne permet de calculer la moyenne de l'écart au carré entre la valeur prédite et la valeur désirée. Cette fonction est définie comme:\n",
        "\n",
        "$J(\\cdot) = \\frac{1}{N}\\sum_{i=1}^{N} (\\hat{y}_{i} - y_i)^{2}$.\n",
        "    \n",
        "<li>**Optimiseur :** <a href=\"http://pytorch.org/docs/master/optim.html#torch.optim.SGD\">`torch.optim.SGD()`</a> qui est une implémentation de SGD.</li>\n",
        "</ul>\n",
        "\n",
        "Nous utilisons une valeur de taux d'apprentissage de 0.001.*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pOZIqkBbpmeo"
      },
      "source": [
        "learning_rate = 0.001\n",
        "\n",
        "# Définir le critère\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Définir l'optimiseur\n",
        "optimizer_lstm = optim.SGD(model_lstm.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqnTlRq8pmew"
      },
      "source": [
        "### Exercice: Entraînement du model\n",
        "\n",
        "Nous utilisons notre objet `train_loader` pour itérer à travers notre ensemble d'entraînement *n_epoch* fois pour faire l'entraînement du modèle. La valeur accumulée de la fonction de coût évaluée sur l'ensemble de validation est sauvegardée à la fin de chaque *epoch* d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odcwM6DyhPYL",
        "outputId": "54234c7a-3642-4c80-8567-3f0f1f352a38"
      },
      "source": [
        "since = time.time()\n",
        "\n",
        "\n",
        "train_loss_history = []\n",
        "valid_loss_history = []\n",
        "\n",
        "num_epochs = 25\n",
        "\n",
        "model_lstm.load_state_dict(init_lstm_weights)\n",
        "\n",
        "print(\"# Début de l'entraînement #\")\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    \n",
        "    train_loss = 0\n",
        "    train_n_iter = 0\n",
        "    \n",
        "    # Mettre le modèle en mode d'entraînement\n",
        "    model_lstm.train()\n",
        "    torch.set_grad_enabled(True)\n",
        "    \n",
        "    # Itérer sur les données d'entraînement\n",
        "    for x, y in train_loader:  \n",
        "\n",
        "        \n",
        "        # Mettre les tenseurs sur le matériel (GPU si possible)\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "\n",
        "        # Réinitisaliser les gradients à zéro\n",
        "        optimizer_lstm.zero_grad()\n",
        "        \n",
        "        # Exécuter la propagation avant\n",
        "        outputs = model_lstm(x)\n",
        "        \n",
        "        # Calculer la perte avec le critère\n",
        "        loss = criterion(outputs, y)\n",
        "        \n",
        "        # Exécuter la propagation arrière\n",
        "        loss.backward()\n",
        "        \n",
        "        # Exécuter le pas d'optimisation\n",
        "        optimizer_lstm.step()\n",
        "        \n",
        "        # Statistiques\n",
        "        train_loss += loss.item()\n",
        "        train_n_iter += 1\n",
        "    \n",
        "    valid_loss = 0\n",
        "    valid_n_iter = 0\n",
        "    \n",
        "    # Mettre le modèle en mode d'évaluation\n",
        "    model_lstm.eval()\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        # Itérer sur les données de validation\n",
        "        for x, y in valid_loader:  \n",
        "        \n",
        "            # Mettre les tenseurs sur le matériel (GPU si possible)\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "        \n",
        "            # Exécuter la propagation avant\n",
        "            outputs = model_lstm(x)\n",
        "            \n",
        "            # Calculer la perte avec le critère\n",
        "            loss = criterion(outputs, y)\n",
        "        \n",
        "            # Statistiques\n",
        "            valid_loss += loss.item()\n",
        "            valid_n_iter += 1\n",
        "    \n",
        "    train_loss_history.append(train_loss / train_n_iter)\n",
        "    valid_loss_history.append(valid_loss / valid_n_iter)\n",
        "    \n",
        "    print(\"Époque {:2d} | Perte d'entraînement = {:.5f} | Perte de validation = {:.5f} \"\n",
        "          .format(epoch+1, (train_loss / train_n_iter), (valid_loss / valid_n_iter)))\n",
        "\n",
        "time_elapsed = time.time() - since\n",
        "\n",
        "print('\\n\\nEntraînement fini en {:.0f}m {:.0f}s'.format(\n",
        "    time_elapsed // 60, time_elapsed % 60))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "# Début de l'entraînement #\n",
            "Époque  1 | Perte d'entraînement = 17.98787 | Perte de validation = 18.18331 \n",
            "Époque  2 | Perte d'entraînement = 17.94840 | Perte de validation = 18.14885 \n",
            "Époque  3 | Perte d'entraînement = 17.88981 | Perte de validation = 18.10186 \n",
            "Époque  4 | Perte d'entraînement = 17.84763 | Perte de validation = 18.02363 \n",
            "Époque  5 | Perte d'entraînement = 17.72345 | Perte de validation = 17.86540 \n",
            "Époque  6 | Perte d'entraînement = 17.48216 | Perte de validation = 17.45209 \n",
            "Époque  7 | Perte d'entraînement = 16.46600 | Perte de validation = 14.88540 \n",
            "Époque  8 | Perte d'entraînement = 9.01503 | Perte de validation = 5.37170 \n",
            "Époque  9 | Perte d'entraînement = 3.88117 | Perte de validation = 2.92120 \n",
            "Époque 10 | Perte d'entraînement = 2.04665 | Perte de validation = 1.69285 \n",
            "Époque 11 | Perte d'entraînement = 1.28445 | Perte de validation = 1.28987 \n",
            "Époque 12 | Perte d'entraînement = 0.91663 | Perte de validation = 0.86155 \n",
            "Époque 13 | Perte d'entraînement = 0.71002 | Perte de validation = 0.71524 \n",
            "Époque 14 | Perte d'entraînement = 0.58138 | Perte de validation = 0.57508 \n",
            "Époque 15 | Perte d'entraînement = 0.48701 | Perte de validation = 0.50489 \n",
            "Époque 16 | Perte d'entraînement = 0.43331 | Perte de validation = 0.43260 \n",
            "Époque 17 | Perte d'entraînement = 0.39259 | Perte de validation = 0.70020 \n",
            "Époque 18 | Perte d'entraînement = 0.36494 | Perte de validation = 0.33495 \n",
            "Époque 19 | Perte d'entraînement = 0.35477 | Perte de validation = 0.54444 \n",
            "Époque 20 | Perte d'entraînement = 0.29649 | Perte de validation = 0.29986 \n",
            "Époque 21 | Perte d'entraînement = 0.32381 | Perte de validation = 0.38094 \n",
            "Époque 22 | Perte d'entraînement = 0.26697 | Perte de validation = 0.22267 \n",
            "Époque 23 | Perte d'entraînement = 0.29723 | Perte de validation = 0.23272 \n",
            "Époque 24 | Perte d'entraînement = 0.26783 | Perte de validation = 0.43922 \n",
            "Époque 25 | Perte d'entraînement = 0.22425 | Perte de validation = 0.25672 \n",
            "\n",
            "\n",
            "Entraînement fini en 2m 5s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aMEzsqBaqGmh"
      },
      "source": [
        "### Visualisation des courbes d'entraînement\n",
        "\n",
        "Visualisez les courbes d'entraînement avec un graphique mettant en valeur la fonction de coût vs. le nombre d'époques pour les ensembles d'entraînement et de validation sur un même graphe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Rw1tIfMhPYN",
        "outputId": "cd877602-934e-4c20-8103-21d707e8543a"
      },
      "source": [
        "# Enregistrer l'historique\n",
        "lstm_train_loss_history = train_loss_history\n",
        "lstm_valid_loss_history = valid_loss_history\n",
        "\n",
        "# Tracer les courbes d'entraînement et de validation\n",
        "xaxis = range(1, num_epochs + 1)\n",
        "plt.plot(xaxis, lstm_train_loss_history, label='entraînement-lstm')\n",
        "plt.plot(xaxis, lstm_valid_loss_history, label='validation-lstm')\n",
        "\n",
        "plt.xlabel('# époques')\n",
        "plt.ylabel('Perte')\n",
        "plt.legend()\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEHCAYAAAC0pdErAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAgAElEQVR4nO3deXxU9bn48c8zk8m+AgkGkrAqewghgpReqsVa9SKKRcVrW2mtSKtevb2/LtpFtNr6otbburRW69ZKbatWRMW1VZFWhLCjqOwkJJBASCB7Zub5/XEmIYEJBMhkksnzfr3mdc58zzkzz2R0Hr7nu4mqYowxxhzNFe4AjDHGdE+WIIwxxgRlCcIYY0xQliCMMcYEZQnCGGNMUJYgjDHGBBUV7gA6U79+/XTw4MHhDsMYY3qM1atX71fV9GDHIipBDB48mMLCwnCHYYwxPYaI7GrvmN1iMsYYE5QlCGOMMUFZgjDGGBNURLVBGGPa19TURHFxMfX19eEOxYRBbGwsWVlZeDyeDl9jCcKYXqK4uJikpCQGDx6MiIQ7HNOFVJUDBw5QXFzMkCFDOnyd3WIyppeor6+nb9++lhx6IRGhb9++J117tAQB4POGOwJjukRvSQ4ffvghL774YrjD6FZO5bu3W0wA9+aAtw6iYiEqBtwxzjYqFqKiA9tYcEcfOScqFjxxgUc8RMc7W098+2XRiRCX6lxvjDmhxYsXc9ZZZzF69OgOX9PQ0MDPfvYz4uLiyM/PZ9CgQSGM8OT8/Oc/5/bbbw967KmnnqKwsJCHHnoo6PF169ZRUlLCxRdfHMoQ27AEAfAf34WmOvDWg6/R2Xqbtw3ga3C29VXgLXOeN9VDU61zXVMtcBILL3niIS4NYlOdhNFmPzWwn+bsJw+EtMFOgjGml1m8eDEzZswImiC8Xi9RUcf+hG3dupV77rmHzMxMNmzY0GMSxImsW7eOwsJCSxBd7cI1k/D5lRiPi9goN7EeNzFRLmfrcREbd+R5bJRTFudxEx/tJjEmioRoN4lRPpJdjSS4Gkh0NREnDUR56wIJpMbZNhyGukqor3S2dQed/YodR/abaoMHmZQJaUOgT+CR1mob36dr/2DGnIZnnnmGBx54gMbGRiZPnsxvf/tbUlJSuOWWW3jllVeIi4vjpZdeYtu2bSxZsoT33nuPu+++mxdeeIHrrruOz33uc/zrX/9i5syZnHXWWdx99900NjbSt29fFi1axJgxY3jqqad47LHHeOihh5g7dy7JyckUFhayd+9eFi5cyOzZswH45S9/yd/+9jcaGhqYNWsWd955Jzt37uTCCy/k85//PCtWrGD8+PF84xvf4I477qCsrIxFixYxadIkampquPnmm9m4cSNer5cFCxZw6aWX8tRTT7FkyRJqa2vZtm0bs2bNYuHChfzwhz+krq6OvLw8xowZw6JFi9r9Gz333HPceeeduN1uUlJSePvtt/npT39KXV0dy5cv57bbbmPz5s3s2LGD0tJSPvvsM+6//35WrFjBa6+9xsCBA3n55ZdPqsdSMJYggHEDU6hp9FLf5Ke+yUddk4+DtY3UN/mob/LT4PXT0OSj3uujydfxmkJMlMtJIDHJJMT0ISHaST7Ow0kysWluYjOc/RiPm3i3jxRqSNLDJPqr6ecvI72phMTaIqIqd8LWf0D13rZvFJsCfYY6yaLvcBhxIQzIh15yv9mcvDtf/oiPSw516muOHpDMHZeMOe45mzdv5q9//Sv/+te/8Hg8fOc732HRokXU1NRwzjnncM899/D973+fxx57jB//+MfMnDmTGTNmtPygA1RWVvLee+8BcPDgQVasWIGI8Ic//IGFCxfyq1/96pj3LS0tZfny5XzyySfMnDmT2bNn8+abb7JlyxZWrlyJqjJz5kyWLVtGTk4OW7du5bnnnuPRRx/l7LPP5s9//jPLly9nyZIl/PznP2fx4sXcc889fPGLX+SJJ56gsrKSSZMmcf755wPOv/bXrl1LTEwMI0aM4Oabb+bee+/loYceYt26dSf8W95111288cYbDBw4kMrKSqKjo7nrrrva3IJasGAB27Zt45133uHjjz9mypQpvPDCCyxcuJBZs2bx6quvctlll3X4+wvGEgTwyyvGd/hcn19bkkhtg4/qBi81jV5nG3hUN/ha7R8pq230UtvopaKmOfn4qPf6qWt0kk/w5cEzAo880uI99E+OZVA2jIqrYJi7nGz2ku4tIbVuD7F71uL6+CVk2UInUeReBeOucGoaxnQD//jHP1i9ejVnn302AHV1dWRkZBAdHc2MGTMAmDhxIm+99Va7r3HVVVe17BcXF3PVVVdRWlpKY2Nju104L7vsMlwuF6NHj2bfvn0AvPnmm7z55ptMmDABgOrqarZs2UJOTg5Dhgxh3LhxAIwZM4bp06cjIowbN46dO3e2XL9kyRLuu+8+wOkltnv3bgCmT59OSkoKAKNHj2bXrl1kZ2d3+O80depU5s6dy5VXXsnll1/e7nkXXXQRHo+HcePG4fP5uPDCCwHaxHk6QpYgROQJYAZQpqpjA2V/BUYETkkFKlU1L8i1O4HDgA/wqmpBqOI8WW6XkBATRUJMFCR23uuqaqCm4qfe66Ou0UlCFTWNlFbVs7eqjr2H6tlbVc+eQ/WsLk1gf7UHGADkt7xOZnQd16Zu4KKGZQx65x545x40ezKSeyWMudxuRxmAE/5LP1RUlWuvvZZf/OIXbcrvu+++ll42brcbr7f9noUJCQkt+zfffDPf/e53mTlzJu+++y4LFiwIek1MzJGOIRr4l5iqctttt3HDDTe0OXfnzp1tzne5XC3PXS5XS2yqygsvvMCIESPaXP/hhx+2ub69z/Pwww/z2GOPAbB06dI2xx555BE+/PBDXn31VfLy8tqtdbSOy+PxtPwNW8d5OkJZg3gKeAj4Y3OBqrakfhH5FVB1nOvPU9X9IYuumxGRlttPKXTsvmGD10fZoYaWxLG3qp6ig7W8sSeT+0um0M9bxqXuf/OVon8xvOh/8S39PhWZXyAm/2qSx19iDd+my02fPp1LL72U//mf/yEjI4OKigoOHz7c7vlJSUnHPV5VVcXAgQMBePrpp08qli9/+cv85Cc/4ZprriExMZE9e/ac1D37L3/5yzz44IM8+OCDiAhr165tqY20x+Px0NTUhMfj4cYbb+TGG28Met62bduYPHkykydP5uWXX6aoqOiEf4tQCFmCUNVlIjI42DFx0tyVwBdD9f69QUyUm+w+8WT3iT/mWJPPz6d7D7O++Fwe3X2Qw7vXM+HgG8zc82+SS/5JzStxbEj+AvuHXsawggsZnZUWhk9gepvRo0dz9913c8EFF+D3+/F4PDz88MPtnj9nzhyuv/56HnjgAZ5//vljji9YsIArrriCgQMHcs4557Bjx44Ox3LBBRewefNmpkyZAkBiYiLPPPMMbre7Q9f/5Cc/4dZbbyU3NxdVZfDgwbzyyivHvWbevHnk5uaSn59/3Ebq733ve2zZsgVVZfr06YwfP56cnBzuvfde8vLyuO222zr8OU+HaPAb353z4k6CeKX5FlOr8mnA/e3dOhKRHcBBnL6jv1fVR4/zHvOAeQA5OTkTd+1qd2rzXq+mwctHxQcp3/g2fbYvJvfQeyRQx0b/YDZd9CJXTxka7hBNCG3evJlRo0aFOwwTRsH+GxCR1e39Foerkfpq4NnjHJ+qqiUikgG8JSKfqOqyYCcGksejAAUFBaHLdhEgISaKScPSYdjVwNXQVEf18kcY994CHn35cUoOf53vfumsXjPa1hhzfF0+1YaIRAGXA39t7xxVLQlsy4AXgUldE10v44kj8Qu3oH2G8f2Ut3nwn1v43+fW0+j1hzsyY0w3EI65mM4HPlHV4mAHRSRBRJKa94ELgE1dGF/v4nIhU75Ddt0n/GpSLX9fs4dvPrWKw/VN4Y7MGBNmIUsQIvIs8AEwQkSKReS6wKE5HHV7SUQGiEhzP6/+wHIRWQ+sBF5V1ddDFacBxv8XxPXhKw2L+eXsXFZsP8AVj3zA3ipbN8CY3ixkCUJVr1bVTFX1qGqWqj4eKJ+rqo8cdW6Jql4c2N+uquMDjzGqek+oYjQB0fFw9nXw6VKuGNLIE3PPpqiilst/+y8+29e13eqMMd2HTfdtHGdfD24PfPAw085K52/zp+D1K1/53b/5YNuBcEdnjAkDSxDGkdQfcq+EdX+G2grGDEjhxRunckZyLNc+sZKX1u0Jd4Sml0lMdKYqKCkpaTMXU2vnnnsuhYWFx32dX//619TWHpkE8+KLL6aysrJTY2zPz3/+8055n3CxBGGOmHKTsy7GqscBGJgax/PzP0deTiq3/GUdv39vG6EcN2NMMAMGDAg6SK6jjk4QS5cuJTU1tTNCOyFLECZyZIyC4efDyked9S6AlHgPf7puEjNyM/nFa5+wYMlH+PyWJMzJ+8EPfsBvf/vblucLFizgzjvvZPr06eTn5zNu3DheeumlY67buXMnY8c6Y23r6uqYM2cOubm5XHXVVdTV1bWc9+1vf5uCggLGjBnDHXfcAcADDzxASUkJ5513Hueddx4AgwcPZv9+Zxaf+++/n7FjxzJ27Fh+/etft7zfqFGjuP766xkzZgwXXHBBm/cJprS0lGnTppGXl8fYsWN5//3320zvfc0117Bz505GjhzJt771LcaOHcs111zD22+/zdSpUznzzDNZuXLlafx1QyOkI6m7WkFBgZ6oumlOYNs78KfLYOZDkP+1lmK/X7n39U94dNl2Lhjdn9/MmUBcdMemJDDdQ5tRtK/9EPZu7Nw3OGMcXHRvu4fXrl3Lrbfe2jJV9+jRo3n99ddJTU0lOTmZ/fv3c84557BlyxZEhMTERKqrq9m5cyczZsxg06ZN3H///WzatIknnniCDRs2kJ+fz4oVKygoKKCiooI+ffrg8/mYPn06DzzwALm5uQwePJjCwkL69esH0PJ8165dzJ07lxUrVqCqTJ48mWeeeYa0tDSGDx9OYWEheXl5XHnllcycOZOvfvWrx3ym5hh/9atfUV9fz49+9CN8Ph+1tbUkJSW1HAcn8QwfPpy1a9cyZswYzj77bMaPH8/jjz/OkiVLePLJJ1m8eHHnfidHOdmR1FaDMG0NPRf6j4UPHqb1/OMul3D7xaNYcMlo3tq8j4VvfBK2EE3PNGHCBMrKyigpKWH9+vWkpaWRmZnJ7bffTm5uLueffz579uxpmY47mGXLlrX8UOfm5pKbm9ty7G9/+xv5+flMmDCBjz76iI8//vi48SxfvpxZs2aRkJBAYmIil19+Oe+//z4AQ4YMIS/PmWh64sSJJ5w6++yzz+bJJ59kwYIFbNy4kaSkpKDnNU8j7nK52p1GvDux9SBMWyJOW8Ti+c7iRGee3+bw3KlDeP2jvawv6pxGPhMmx/mXfijNnj2b559/nr179zJnzhwWLVpEeXk5q1evxuPxMHjwYOrrjz/+JthUMDt27OC+++5j1apVpKWlMXfu3BO+zvHunhw9XXddXR1FRUVccsklAMyfP5/58+e3nDNt2jSWLVvGq6++yte+9jW+973v8fWvf/24r9veNOLdidUgzLHGfsVZ4vSD4IunD0tPZFt5jTVYm5M2Z84c/vKXv/D8888ze/ZsqqqqyMjIwOPx8M4773CiyTanTZvWMgvqpk2b2LBhAwCHDh0iISGBlJQU9u3bx2uvvdZyTXvTZE+bNo3FixdTW1tLTU0NL774Iv/xH//R7ntnZ2ezbt061q1b1yY5AOzatYuMjAyuv/56rrvuOtasWQMcmd67p7IEYY4VFQ2T5sH2d2DvsbOcDE1PpKquiYqaxjAEZ3qyMWPGcPjwYQYOHEhmZibXXHMNhYWFFBQUsGjRIkaOHHnc67/97W9TXV1Nbm4uCxcuZNIkZ5q28ePHM2HCBMaMGcM3v/lNpk6d2nLNvHnzuOiii1oaqZvl5+czd+5cJk2axOTJk/nWt751wvUc2vPuu++Sl5fHhAkTeOGFF7jlllta3js3N5drrrnmlF433KyR2gRXdxDuHwOjL4VZv2tz6N1Py5j75Cr+dsMUJg2xFep6Cpvu21gjtekccWkw4auw8Tk4VNrm0LB0Z3DQ9vLqcERmjOkiliBM+86ZD36vMy6ilQGpccREudhmCcKYiGYJwrSvz1AYNQMKn4DGmpZit0sY0i+BbeU1x7nYGNPTWYIwxzflZqivhLVt188dlp5ot5h6oEhqczQn51S+e0sQ5vhyJkPW2bDiYfD7WoqHpSewu6KWBq/vOBeb7iQ2NpYDBw5YkuiFVJUDBw4QGxt7UtfZQDlzYlNugueuhU9ehdEzAaerq19h94FazuwffNSo6V6ysrIoLi6mvLw83KGYMIiNjSUrK+ukrrEEYU5s1CWQOsgZOBdIEM09mbaVV1uC6CE8Hg9DhgwJdximB7FbTObEXG445ztQ9CEUrQJgaHoCgDVUGxPBQrkm9RMiUiYim1qVLRCRPSKyLvC4uJ1rLxSRT0Vkq4j8MFQxmpMw4asQm9Iy/UZCTBRnJMdaV1djIlgoaxBPARcGKf8/Vc0LPJYefVBE3MDDwEXAaOBqERkdwjhNR8QkwsRvwOYlcHAnAMMyrKurMZEsZAlCVZcBFadw6SRgq6puV9VG4C/ApZ0anDk1k28AccGKRwAY2s/p6mq9YoyJTOFog7hJRDYEbkGlBTk+EChq9bw4UGbCLXkAjJ0Na/8EdZUMS0/gcL2X8uqGcEdmjAmBrk4QvwOGAXlAKfCrIOccO9k7tPtPVBGZJyKFIlJo3fe6wOdugsZqWPM0Q5t7MpXZbSZjIlGXJghV3aeqPlX1A4/h3E46WjGQ3ep5FlBynNd8VFULVLUgPT29cwM2xzpjHKSPgl0fMCwjMGnffmuoNiYSdWmCEJHMVk9nAccuNgCrgDNFZIiIRANzgCVdEZ/poLRBUFVEZnIscR631SCMiVAhGygnIs8C5wL9RKQYuAM4V0TycG4Z7QRuCJw7APiDql6sql4RuQl4A3ADT6jqR6GK05yClGzY9QGuwKR9VoMwJjKFLEGo6tVBih9v59wS4OJWz5cCx3SBNd1EajY0VEF9FcMyEllXdDDcERljQsBGUpuTlxJoIqosYmi/BIoP1lHfZJP2GRNpLEGYk5ea42yrihiWkYgq7Dxg7RDGRBpLEObktapBDGuek8kaqo2JOJYgzMlLSAd3DFTtZkg/J0HY4kHGRB5LEObkuVyQkgWVRcRHRzEwNc4m7TMmAlmCMKcmNRuqnBlRhqbbpH3GRCJLEObUpGRDpZMgmtentkn7jIksliDMqUnNgZoyaKpnWHoCNY0+9h2ySfuMiSSWIMypae7JVFXcMmmfNVQbE1ksQZhTk9qcIHa3WZ/aGBM5LEGYU9NqLET/5BgSot3WUG1MhLEEYU5N8gBndbmqIkSEoemJVoMwJsJYgjCnxu2BpAEtPZmGpiew3WoQxkQUSxDm1LUaCzEsPZE9lXXUNdqkfcZECksQ5tQdNRYCbHU5YyKJJQhz6lKz4dAe8HkZ2jxpn91mMiZiWIIwpy4lG9QHh0sZ0i8BERsLYUwksQRhTl3LWIgiYj3uwKR9VoMwJlJYgjCnLiWwcNBRczIZYyJDyBKEiDwhImUisqlV2S9F5BMR2SAiL4pIajvX7hSRjSKyTkQKQxWjOU0pWc62ajfQnCBq8Ptt0j5jIkEoaxBPARceVfYWMFZVc4HPgNuOc/15qpqnqgUhis+cruh4iO/XZixEXZOP0kP1YQ7MGNMZQpYgVHUZUHFU2Zuq6g08XQFkher9TRc5aiwEWEO1MZEinG0Q3wRea+eYAm+KyGoRmXe8FxGReSJSKCKF5eXlnR6kOYE2YyGa16e2BGFMJAhLghCRHwFeYFE7p0xV1XzgIuBGEZnW3mup6qOqWqCqBenp6SGI1hxXag5UFYMq6UkxJMVEsX2/9WQyJhJ0eYIQkWuBGcA12s4SZKpaEtiWAS8Ck7ouQnNSUrLBWwc1+51J+zJs0j5jIkWXJggRuRD4ATBTVWvbOSdBRJKa94ELgE3BzjXdQKt1IQCG9UtgW5nVIIyJBKHs5vos8AEwQkSKReQ64CEgCXgr0IX1kcC5A0RkaeDS/sByEVkPrAReVdXXQxWnOU2t1oUAGJaRyN5D9VQ3eI9zkTGmJ4gK1Qur6tVBih9v59wS4OLA/nZgfKjiMp2s1WhqgKH9nIbqHeU1jMtKCVdUxphOYCOpzemJTYXopDY1CLBZXY2JBJYgzOkRaTMWYlDfeFxiXV2NiQSWIMzpazUWIibKTXafeLZZV1djejxLEOb0pWa39GICZ0S11SCM6fksQZjTl5IN9VVQfwhwGqp37LdJ+4zp6SxBmNN3VE+mYRmJNHj97KmsC2NQxpjTZQnCnL6j1oVo7upqI6qN6dksQZjTF6QGAbDdVpczpkezBGFOX0IGuKOh0mmo7psQTUqcx2oQxvRwliDM6XO5nNXlAjUIEWFoeoIlCGN6OEsQpnO0GgsBR5YfNcb0XJYgTOdoNZoanOVHyw43cLi+KYxBGWNOhyUI0zlScqB6HzQ561EfWX7UahHG9FSWIEznaO7JdGgPcCRBWDuEMT2XJQjTOVrWhXB6MuX0icftEksQxvRgliBM5zhqLER0lItBfeLtFpMxPZglCNM5kgeCuNr0ZLKursb0bJYgTOdweyAps01PpmHpiezcX4vPJu0zpkcKaYIQkSdEpExENrUq6yMib4nIlsA2rZ1rrw2cs0VErg1lnKaTBBkL0ejzU3ywNoxBGWNOVahrEE8BFx5V9kPgH6p6JvCPwPM2RKQPcAcwGZgE3NFeIjHdyFHrQgxNt0n7jOnJOpQgRCReRH4iIo8Fnp8pIjNOdJ2qLgMqjiq+FHg6sP80cFmQS78MvKWqFap6EHiLYxON6W5SsuFQCfh9gI2FMKan62gN4kmgAZgSeF4M3H2K79lfVUsBAtuMIOcMBIpaPS8OlJnuLDUb/F44XApAWkI0afE2aZ8xPVVHE8QwVV0INAGoah0gIYsq+GsHbekUkXkiUigiheXl5SEMyZxQy7oQRy0/ajUIY3qkjiaIRhGJI/AjLSLDcGoUp2KfiGQGXicTKAtyTjGQ3ep5FlAS7MVU9VFVLVDVgvT09FMMyXSK5rEQx0zaZzUIY3qijiaIBcDrQLaILMJpXP7BKb7nEqC5V9K1wEtBznkDuEBE0gKN0xcEykx3lpLlbI9qqN5f3UhVrU3aZ0xP06EEoapvApcDc4FngQJVfedE14nIs8AHwAgRKRaR64B7gS+JyBbgS4HniEiBiPwh8H4VwM+AVYHHXYEy051FJ0B832NqEADb9lstwpieJqojJ4nIP1R1OvBqkLJ2qerV7Rw65jpVLQS+1er5E8ATHYnPdCMpx077DbCtrJr8HOupbExPctwEISKxQDzQL3Crp7nxOBkYEOLYTE+Umg1ln7Q8ze4Tj8ctbN9vDdXG9DQnqkHcANyKkwxWcyRBHAIeDmFcpqdKyYEtb4MqiOBxu8jpE8+2MrvFZExPc9wEoaq/EZGHgNtV9WddFJPpyVKzwVsHNfsh0elV5nR1tQRhTE9zwkZqVfUBF3dBLCYSNK8L0aon07CMRHZX1NLk84cpKGPMqehoN9c3ReQrIhLKwXEmEgQZCzG0XwJNPqWowibtM6Yn6VAvJuC7QALgE5HmUdSqqskhi8z0TCltFw4CpwYBsKWsmqGBbq/GmO6vo+MgklTVpaoeVU0OPLfkYI4VlwbRiW1qEKMzk4lyCeuKKsMYmDHmZHV0NlcRka+KyE8Cz7NFZFJoQzM9ksgxYyFiPW7GDEhmza6DYQzMGHOyOtoG8VucmVz/K/C8GuvmatqT2nbhIIAJOWmsL660hmpjepCOJojJqnojUA8QWKMhOmRRmZ4tpe3CQQD5g9Kob/LzSenhMAVljDlZHU0QTSLi5shsrumA/VPQBJeaDfVVUH+opWjiIGeajdW7bEotY3qKjiaIB4AXgQwRuQdYDvw8ZFGZni1IT6YBKbH0T45hzW5rqDamp+hQN1dVXSQiq3Em2RPgMlXdHNLITM+V2rxwUBH0HwOAiDBxUBqrraHamB6jI5P1zQeGAxuB36uqtysCMz1YkBoEQH5OGks37qXsUD0ZybFhCMwYczJOdIvpaaAAJzlcBNwX8ohMz5fYH9zRbZYeBaehGmDNbqtFGNMTnOgW02hVHQcgIo8DK0MfkunxXC5IHnhMDWLMgGSi3S5W7zrIhWMzwxScMaajTlSDaFkn0m4tmZMSZCxETJSbcVkp1lBtTA9xogQxXkQOBR6HgdzmfRE5dIJrTW+WknNMDQIgPyeVjcVVNHh9YQjKGHMyjpsgVNUdmHupef6lKJuLyXRIajZU74Om+jbFEwel0ejz81GJ/fvCmO6uo+MgOo2IjBCRda0eh0Tk1qPOOVdEqlqd89OujtOcpuaeTIf2tCluXpfa5mUypvvr6HTfnUZVPwXyAAKjs/fgDMI72vuqOqMrYzOdqGVdiN3Qd1hLcUZyLFlpcdaTyZgeoMtrEEeZDmxT1V1hjsN0tnbGQoBTi1i96yCq2sVBGWNORrgTxBzg2XaOTRGR9SLymoiMae8FRGSeiBSKSGF5eXloojQnL3kgIMf0ZAKnoXrfoQZKquqPvc4Y022ELUGISDQwE3guyOE1wCBVHQ88CCxu73VU9VFVLVDVgvT09NAEa05eVDQkZQatQUwc1AfApt0wppsLZw3iImCNqu47+oCqHlLV6sD+UsAjIv26OkBzmoKMhQAYmZlErMdlDdXGdHPhTBBX087tJRE5Q0QksD8JJ84DXRib6QxB1oUA8LhdjM9KZa01VBvTrYUlQYhIPPAl4O+tyuaLyPzA09nAJhFZjzPV+By1Fs2eJzUbDpWA/9hBcfmD0vio5BD1TTZgzpjuKiwJQlVrVbWvqla1KntEVR8J7D+kqmNUdbyqnqOq/w5HnOY0pWSD3wuHS485NDEnDa9f2VBcFeRCY0x3EO5eTCaStV4X4igTclIBa6g2pjuzBGFC5zhjIfomxjCkX4INmDOmG7MEYUKn9WjqICbkpLLGBvFJ2HsAABcCSURBVMwZ021ZgjChE50A8X2D1iDAmbjvQE0juytquzgwY0xHWIIwoZUSfCwEHJm4z9ohjOmeLEGY0ErNbrcGcVb/JBJjoqwdwphuyhKECa2UHKcGEaSdwe0S8rJTWb3LVpgzpjuyBGFCKzUbvHVQG3wgfH5OKp/uPUR1g61oa0x3YwnChFbK8Xsy5Q9Kw6+wochqEcZ0N5YgTGiltj8WAmBCtjVUG9NdWYIwodVSgwieIFLiPZyZkWgN1cZ0Q5YgTGjFpUF0Yrs1CHC6u67ZXYnfbwPmjOlOLEGY0BI57lgIcAbMVdU1sX1/TRcGZow5EUsQJvRSg68L0Sx/kDNxny0gZEz3YgnChN4JahBD+yWSEuexdghjuhlLECb0UrOhvhIaDgc97HIJE3JSrSeTMd2MJQgTeifoyQTOAkJbyqqpqmvqoqCMMSdiCcKEXuogZ7v/03ZPyR/kjIewdaqN6T7CliBEZKeIbBSRdSJSGOS4iMgDIrJVRDaISH444jSdIHM8JA2AlX9o95Tx2am4BNbsthHVxnQX4a5BnKeqeapaEOTYRcCZgcc84HddGpnpPFHR8LmbYNdyKFoZ9JTEmChGnJFsPZmM6UbCnSCO51Lgj+pYAaSKSGa4gzKnKP9aZ9Dc8v9r/5ScVNYVVeKzAXPGdAvhTBAKvCkiq0VkXpDjA4HWrZrFgTLTE8UkwqQb4NOlULY56CkTB6VR3eBlS1nw3k7GmK4VzgQxVVXzcW4l3Sgi0446LkGuOeafliIyT0QKRaSwvLw8FHGazjJpHnji4V+/CXrYVpgzpnsJW4JQ1ZLAtgx4EZh01CnFQHar51lASZDXeVRVC1S1ID09PVThms6Q0Ne51bTxuaDTfw/qG0/fhGjW2AJCxnQLYUkQIpIgIknN+8AFwKajTlsCfD3Qm+kcoEpVS7s4VNPZPneTs/33Q8ccEhEm5KTZiGpjuolw1SD6A8tFZD2wEnhVVV8XkfkiMj9wzlJgO7AVeAz4TnhCNZ0qJQtyr4I1f4Sa/cccnjgojR37a6ioaQxDcMaY1qLC8aaquh0YH6T8kVb7CtzYlXGZLjL1Flj3Z/jwEfjij9scys85MnHf+aP7hyM6Y0xAd+7maiJV+ggY+Z+w8tFj5mfKzUolyiV2m8mYbsAShAmPz38X6qtg9VNtiuOi3YwekGw9mYzpBixBmPDImghDpsEHD4O3oc2h/Jw0NhRX0eTzhyk4YwxYgjDh9Pn/gcOlsP4vbYrzB6VR1+Tjk1IbMGdMOFmCMOEz9DzIzHMGzvl9LcUtDdXWDmFMWFmCMOEj4tQiKrbB5iUtxQNT4+ifHGMJwpgwswRhwmvUJdB3uDOJnzozqYgI+TlpFO48iKpN3GdMuFiCMOHlcjvjIkrXw7Z/thSfNzKDPZV1LFl/zOwqxpguYgnChF/uVZCU2WYq8K/kZzE+O5W7Xv6YylobVW1MOFiCMOEXFQNTboKd70Oxs7ig2yX8YtY4Kuua+MXST8IcoDG9kyUI0z1MvBZiU9vUIkYPSOZbnx/CXwuL+HD7gTAGZ0zvZAnCdA8xSc56EZ+8AuWfthTfcv6ZZKXFcfuLG2nw+o7zAsaYzmYJwnQfk+dDVFybBYXio6O4+7KxbCuv4ZF3t4cxOGN6H0sQpvtI6OvcatrwV6g8strsuSMyuGT8AB5+ZyvbyqvDGKAxvYslCNO9TAksKPTBw22KfzJjFLEeFz96caONjTCmi1iCMN1LajaMuxLWPA01RxqmM5Ji+eFFo1ixvYLnVxeHMUBjeg9LEKb7mXoLNNXCyt+3KZ5zdjYFg9K4Z+lmDlQ3tHOxMaazWIIw3U/GSBg5A1Y8AuWftRS7XMIvLh9HTYOXe17dHMYAjekdLEGY7umCnzkD6P54KRzc1VJ8Zv8kbpg2jL+v3cPyLceuaW2M6TxdniBEJFtE3hGRzSLykYjcEuScc0WkSkTWBR4/7eo4TZj1GQpfexGaauBPl8HhfS2HbvricAb3jedHizdS32RjI4wJlXDUILzA/6rqKOAc4EYRGR3kvPdVNS/wuKtrQzTdwhlj4ZrnneTwp1lQWwFArMfNPbPGsetALQ/9c2uYgzQmcnV5glDVUlVdE9g/DGwGBnZ1HKaHyJ4EcxbBgS2w6ApocMZBTB3ej8vzB/LIe9v4bJ+tPGdMKIS1DUJEBgMTgA+DHJ4iIutF5DURGXOc15gnIoUiUlheXh6iSE1YDTsPZj8BJWvhL/8FTfUA/OjiUSTFRnHb3zfi99vYCGM6W9gShIgkAi8At6rqoaMOrwEGqep44EFgcXuvo6qPqmqBqhakp6eHLmATXqMugUsfhh3vwfPfBJ+Xvokx3H7xKFbvOsizq3aHO0JjIk5YEoSIeHCSwyJV/fvRx1X1kKpWB/aXAh4R6dfFYZruJu9quGghfPoqvHQj+P3MnpjFlKF9ufe1Tyg7XB/uCI2JKOHoxSTA48BmVb2/nXPOCJyHiEzCidPmezYw+QY478ew4S/w+g8Q4J5ZY2lo8nPXyx+HOzpjIkpUGN5zKvA1YKOIrAuU3Q7kAKjqI8Bs4Nsi4gXqgDlqE/CYZtP+HzRUwb8fhNgUhn7xx9x43nD+7+3PuDRvH18a3T/cERoTEbo8QajqckBOcM5DwENdE5HpcUTgSz+D+ipY9kuISWb+uTeydGMp859ZzfwvDOW/p59JTJQ73JEa06PZSGrTM4nAjF/DmFnw1k+IWf8n/nbDFGZNGMjD72xjxgPLWVdUGe4ojenRLEGYnsvlhlmPwvAvwcu3krJ9CfddMZ4nv3E21Q1eLv/tv/jFa5tttLUxp8gShOnZoqLhyj9CzhT4+zxY9TjnDY7jjf+ZxpUF2fz+ve1c/MD7rN51MNyRGtPjWIIwPV90PPzXX2DABHj1u/DLM0l++XruHb2bZ+aOp6HJz+xH/s3dr3xMXaPVJozpKImkzkEFBQVaWFgY7jBMuPj9ULwSNj4PH70ItfshJoWmETP4U3UBd3+cTk7fRBbOHs+kIX3CHa0x3YKIrFbVgqDHLEGYiOTzwo53nWSx+RVoPExjbDqLvZN5tnYS4ydP5/sXjSQ+Ohw9vY3pPixBmN6tqQ4+ewM2PY9+9ibia2CXP4P3or9A7kXXkTdxSrgjNCZsLEEY06y+Cja/QtXKP5NY+m/c+NkWNYy9Qy4n5wtfJzsrJ9wRGtOlLEEYE0RdRSmFS/9A5o4XGe7bRpO6WeUp4MDwrzD8819h5MC+BGZ8MSZiWYIw5gT2bllD2fInySl6mVT/QSo0kXc8X+DQiCsYP+kL5GWn4XJZsjCRxxKEMR3l81K16Q0qVzxNZuk/iaaJT/1ZvOk5j9qRs/n8hLFMGtIHj9t6iJvIYAnCmFNRd5C6tc9Rt+oZ+hxcj0+FZf5c3nZNxd9nOMmZw8jKHsyozGRGnJFEUqwn3BEbc9IsQRhzuvZvoWnNn/Gt/TOxdXtbiuvVQ7GmU6TpVEZn4k3OISZ9MGkDz2Tg4JHkDBxIlE0aaLoxSxDGdBa/D/Z/BpW70YM7qdm3ndqyHVC5i4TaEhL8bRdHrNFYyqP6Ux07gIbEgfiTs/D0HURixlBSBwylT3oWrp5wu+pQKez+AErXQb8RMOIiiLfBhpHgeAnCRgkZczJcbsgYBRmjECAx8GhRX0XD/l3s3fUpB0u20lC+nahDRSTXlpJdvYGUfTWw5cjpDeqhzJVOhecMauMyaUrKwpWWTUy/ISSkppOc2peUtH4kJiYjri5KJH4/lH/iJISiD51tZWBJV3GB+kHcMOhzzlKwI/8TUrK6JjbTpawGYUwX8fuVior9VOzZSnXZDhr374JDRcRU7yGpvoQ+TfvoQ1XQa73qoloSqHUl0uBOoMmTjDc6GWKScMWl4o5PwZOQRlR8Kp6ENKIT+xCT1IeYxDTccakQnQjtJZimOtiz5khCKPrQGS8CkJABOeccefQfB2UfOaPTP3nFSSTgzIM16hIYeQmkn3Vyf5i6Stj3EezbFHh85NTUBubDwIkwsAD6ndV+/Oa02C0mY3oIf0MtB/dup6p0B3WH9lNffRBvzUF8dVVQV4Wr8RDupsPEeKuJ81WTQA3J1JIox1+P24eLGuKpcSVQ50qk3p1IQ1QSab4Ksho+I0q9AByIG8K+1DwO9JlAVb8CvCmDiI12E+NxExvlJtbjIjrKRbTb2cZWbSdh++vEbltKVOkaALTfWcjIGTBqBgzId9buAOdHv2K7kwT2bjqSFKqKjgQa1wfOGOvsl6yDhsAtu+gkGJAHWQVHkkZy5qn/oX1NTmKMSToSX7g01TmJdt9HzuPgLkjNdpJiv7MgfQQkpIcsTksQxkSoBq+PippGKg7XUl1VQWN1Jd7aSnx1B9HaKrS+CleDk1g8jVV4mg4T7T1MrK+aeN9hqkhgHSNY6R/BqqbhlPkSTjmWMzjAl9yr+bJ7FefIZqLEz176sl5GkkUZQ3U3cTQATsIqcg1km2sIO9xD2OEezE7PUCpdfXC5XHjcQmK0i2GuUkb4PmNow6dk120mo/Yz3OrMyFsf15/qfuNp6D8Bf+YEcHnw1x6EugqkvhKpP4irvhJ3fSVRDZVENVbiaawiurEKj68GgMaoRGoTsmlIGoQ3ZTCaNgRX3yF40ocR1y+H+GhPh8a/qCpev9Lo9dPo9dPk89MQ2LpE8ES5iHYJMTV7iDmwGc/+j3GVfQRlH8OBrc5tO4CoOEjNgUN7oLH6yBvEpjhtP/3OcmpozckjbbBz2/M0dLsEISIXAr8B3MAfVPXeo47HAH8EJgIHgKtUdeeJXtcShDGnx+dXGrw+6pv81Df5Ag8/9V5nv6HpyA9f86PRpzQFypp/HF31lQypWMZZB98js/ZT9kdnURIzjD2xwyiJHkZJdA6NEoNfFb9fna06P7R+dRJfdYOPmgYvNQ1eqgNbjzYyWnYx3rWNPNdWxss2hrj2Bf0sXnVRSSJVmkAliVRqIpUkUKXOfgMeBsh+Bss+cmQfWbIfjxyZDr5Bo5weapLJXvcZlEUN4LA7Fb/fh8/nx+/z4ff7Ub8Xv9+PoLhwtm78uFDc+MiRMka6djNCikiWupbX36UZfMYgtsogdrgGs9M9mHJPJlFRHmKjXGS5DzKYPQzyF5HlKyazaTcZDbtJ8h448n2Jh8OJg6lNHcGAbz5zSrWMbpUgRMQNfAZ8CSgGVgFXq+rHrc75DpCrqvNFZA4wS1WvOtFrW4IwJnKpKvVN/pZk0bxtOFyOp2wjIoI/Ng2JTYP4NFwxSXg87pbbYR63UzOJdjv7LhHqmnwtr1NTX4/vYDGugzuIOrSL2MO7iK/eTXJdMakNe4jx1504yCAao5KoTDqTisQz2R8/nLL44eyNGUqtxNEYSKqNPj9NgW2j19+SmOsCSbpl2+gjuukQ2f49DHftYZiUMkz2kBjlZ8od751SfN2tF9MkYKuqbgcQkb8AlwIftzrnUmBBYP954CEREY2k+2HGmJMiIsRFu4mLdpOeFNPqSF9g5Cm9ZgpHD248AwjyW6kKNeVQW+Hc0hGX8691cTk9usTlPFyt9kVA3ETHJJEhQsYpRRicz69tEofXF5qfxnAkiIFAq1YpioHJ7Z2jql4RqcL5r2B/l0RojDGtiUBihvPoBtwuISEmioSY0P6Eh6PfWLCbZEenv46c45woMk9ECkWksLy8/LSDM8YY4whHgigGsls9zwJK2jtHRKKAFKAi2Iup6qOqWqCqBenp6SEI1xhjeqdwJIhVwJkiMkREooE5wJKjzlkCXBvYnw3809ofjDGma3V5G0SgTeEm4A2cbq5PqOpHInIXUKiqS4DHgT+JyFacmsOcro7TGGN6u7DMxaSqS4GlR5X9tNV+PXBFV8dljDHmCJvcxBhjTFCWIIwxxgQVUXMxiUg5sAvoR+8eM9GbP7999t6rN3/+0/nsg1Q1aBfQiEoQzUSksL2h471Bb/789tl752eH3v35Q/XZ7RaTMcaYoCxBGGOMCSpSE8Sj4Q4gzHrz57fP3nv15s8fks8ekW0QxhhjTl+k1iCMMcacJksQxhhjgoq4BCEiF4rIpyKyVUR+GO54upKI7BSRjSKyTkQifmk9EXlCRMpEZFOrsj4i8paIbAls08IZY6i089kXiMiewPe/TkQuDmeMoSIi2SLyjohsFpGPROSWQHlv+e7b+/yd/v1HVBtER5YzjWQishMoUNVeMVhIRKYB1cAfVXVsoGwhUKGq9wb+gZCmqj8IZ5yh0M5nXwBUq+p94Ywt1EQkE8hU1TUikgSsBi4D5tI7vvv2Pv+VdPL3H2k1iJblTFW1EWheztREIFVdxrHrhFwKPB3Yfxrnf5yI085n7xVUtVRV1wT2DwObcVah7C3ffXufv9NFWoIItpxpSP5w3ZQCb4rIahGZF+5gwqS/qpaC8z8SdOpSwD3BTSKyIXALKiJvsbQmIoOBCcCH9MLv/qjPD538/UdagujwUqURaqqq5gMXATcGbkOY3uN3wDAgDygFfhXecEJLRBKBF4BbVfVQuOPpakE+f6d//5GWIDqynGnEUtWSwLYMeBHnlltvsy9wj7b5Xm1ZmOPpMqq6T1V9quoHHiOCv38R8eD8OC5S1b8HinvNdx/s84fi+4+0BNGR5UwjkogkBBqsEJEE4AJg0/Gvikitl6u9FngpjLF0qeYfx4BZROj3LyKCs+rkZlW9v9WhXvHdt/f5Q/H9R1QvJoBA165fc2Q503vCHFKXEJGhOLUGcFYK/HOkf3YReRY4F2eq433AHcBi4G9ADrAbuEJVI64xt53Pfi7O7QUFdgI3NN+TjyQi8nngfWAj4A8U345zH743fPftff6r6eTvP+IShDHGmM4RabeYjDHGdBJLEMYYY4KyBGGMMSYoSxDGGGOCsgRhzEkSkSgRuUlEYsIdizGhZAnCGEBEfiEi54rIZcebBTjQB/3XwAZVbei6CI3pepYgjHFMxulH/wWcPuZBqeOmwGR5xkQ0SxCmVxORX4rIBuBs4APgW8DvROSnQc5NF5EXRGRV4DE1UL5ARP4kIv8MrEVwfaBcAq+/KbBOx1Wtyh8SkY9F5FURWSoiswPHdopIv8B+gYi8G9hPCEzAtkpE1orIpYHyMSKyMjD//wYROTPkfzTTa0SFOwBjwklVvycizwFfA74LvKuqU9s5/TfA/6nqchHJAd4ARgWO5QLnAAnAWhF5FZiCM7J1PM6I51UisixQPgIYB/QHPgaeOEGoPwL+qarfFJFUYKWIvA3MB36jqosC08u4T/6vYExwliCMcaZLXgeMxPmxbs/5wGinGQKA5Ob5r4CXVLUOqBORd3AmSvs88Kyq+nAmknsPp6YyrVV5iYj8swMxXgDMFJH/F3geizOlxAfAj0QkC/i7qm7p2Ec25sQsQZheS0TygKdwZv3dD8Q7xbIOmBL4wW/NFaw8kDCOnrNGCT79fOvjwXg5cus3tvXbAF9R1U+POn+ziHwI/Cfwhoh8S1U7knCMOSFrgzC9lqquU9U8nGVqRwP/BL6sqnlBkgPAm8BNzU8CCabZpSISKyJ9cSbNWwUsA64SEbeIpOPUHFYGyucEyjOB81q9zk5gYmD/K63K3wBuDvSiQkQmBLZDge2q+gDObKa5J/+XMCY4SxCmVwv8cB8MzKE/8gTrl/83UBBoDP4Y5/5/s5XAq8AK4GeBtTleBDYA63GSz/dVdW+gfAvObJy/A95r9Tp3Ar8RkfcBX6vynwEeYIOIbAo8B7gK2BSo9YwE/niyfwNj2mOzuRpzmkRkAaexWLyIPAW8oqrPd2Zcxpwuq0EYY4wJymoQxhhjgrIahDHGmKAsQRhjjAnKEoQxxpigLEEYY4wJyhKEMcaYoCxBGGOMCer/A7F8yharjceoAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7PCowiQxpcHv"
      },
      "source": [
        "## Analyse des résultats\n",
        "\n",
        "Analysons l'erreur générée par notre modèle sur notre ensemble d'évaluation.  Nous allons comparer l'erreur quadratique moyenne sur l'ensemble d'évaluation pour les modèles RNN et LSTM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KaoQgOOZKL98"
      },
      "source": [
        "### Exercice: Comparaisons des courbes d'entraînenement\n",
        "\n",
        "Nous allons comparer sur un même graphique l'erreur en fonction du nombre d'*epochs* lors de notre entraînement sur nos ensembles d'entraînement et de validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yl8hcNhnKKOS",
        "outputId": "68b4eede-8802-4471-f913-046a526833f3"
      },
      "source": [
        "# Tracer les courbes d'entraînement et de validation\n",
        "xaxis = range(1, num_epochs + 1)\n",
        "\n",
        "plt.plot(xaxis, rnn_train_loss_history, label='entraînement-rnn')\n",
        "plt.plot(xaxis, rnn_valid_loss_history, label='validation-rnn')\n",
        "\n",
        "plt.plot(xaxis, lstm_train_loss_history, label='entraînement-lstm', linestyle='--')\n",
        "plt.plot(xaxis, lstm_valid_loss_history, label='validation-lstm', linestyle='--')\n",
        "\n",
        "plt.xlabel('# époques')\n",
        "plt.ylabel('Perte')\n",
        "plt.legend()\n",
        "plt.ylim([0,2])\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhGBAmabKr8A"
      },
      "source": [
        "Remarquez qu'il y a une différence entre les résultats des RNN et des LSTM.\n",
        "\n",
        "**Questions**\n",
        "\n",
        "1.   Quel réseau fonctionne mieux sur notre ensemble de validation? Pourquoi?\n",
        "2.   Supposons que vous vouliez augmenter l'écart entre les performances du RNN et du LSTM. Comment modifieriez-vous les données ?\n",
        "\n",
        "... # à compléter.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SeWGSfEOLdsM"
      },
      "source": [
        "### Exercice: Calcul de l'erreur sur l'ensemble d'évaluation\n",
        "\n",
        "Calculez l'erreur quadratique moyenne sur l'ensemble d'évaluation pour les RNN et les LSTM et imprimez les deux valeurs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "72z2Xob3MRYB",
        "outputId": "d31ab458-8046-41de-aae4-00dba5442ead"
      },
      "source": [
        "# Mettre les données séquentielles sur le matériel (GPU si possible)\n",
        "xtest = xtest.to(device)\n",
        "\n",
        "# Mettre les étiquettes sur le matériel (GPU si possible)\n",
        "ytest = ytest.to(device)\n",
        "\n",
        "# Calculer les valeurs prédites par le RNN pour l`ensemble d'évaluation\n",
        "ypred_rnn = model_rnn(xtest)\n",
        "\n",
        "# Calculer les valeurs prédites par le LSTM et LSTM pour l`ensemble d'évaluation\n",
        "ypred_lstm = model_lstm(xtest)\n",
        "\n",
        "# Calculer la perte du modèle à base de RNN avec le critère\n",
        "loss_test_rnn = criterion(ypred_rnn, ytest)\n",
        "\n",
        "# Calculer la perte du modèle à base de LSTM avec le critère\n",
        "loss_test_lstm = criterion(ypred_lstm, ytest)\n",
        "\n",
        "\n",
        "print(\"L'erreur quadratique moyenne d'évaluation du RNN est %2.3f\" % float(loss_test_rnn))\n",
        "print(\"L'erreur quadratique moyenne d'évaluation du LSTM est %2.3f\" % float(loss_test_lstm))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "L'erreur quadratique moyenne d'évaluation du RNN est 0.451\n",
            "L'erreur quadratique moyenne d'évaluation du LSTM est 0.254\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hPZH71eCpcH0"
      },
      "source": [
        "### Exercice: Vérification visuelle des résultats\n",
        "\n",
        "Pour mieux comprendre nos modèles, nous examinerons leurs résultats (c'est-à-dire leurs prédictions avant de calculer le coût). Nous définissons une fonction `print_sequence()` qui aidera à visualiser les prédictions faites par nos modèles. \n",
        "\n",
        "Cette fonction prend en entrée les tenseurs X et Y pour échantillonner et imprimer un exemple de séquence ainsi que la différence absolue entre une valeur de Y et la valeur réelle de la somme de X.\n",
        "\n",
        "Rappelons que nous avons standardisé nos exemples. Pour ces visualisations, nous souhaitons utiliser la pré-normalisation des données originales. Pour ce faire, il faut réutiliser `mean` + `std` tel que \n",
        "\n",
        "$xtest\\_unstd = xtest*std + mean$ et $ytest\\_unstd = ytest*std+seq\\_len*mean$\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfCvsGXNpcH2"
      },
      "source": [
        "def print_sequence(X, Y, idx=0):\n",
        "    \"\"\"Imprimer et vérifier les séquences d'opérations.\n",
        "    Args:\n",
        "      X: torch.Tensor.\n",
        "      Y: torch.Tensor.\n",
        "      idx: indice de la séquence à vérifier.\n",
        "    \"\"\"\n",
        "    x = X[idx].numpy()\n",
        "    y = Y[idx].numpy()\n",
        "    for i, xi in enumerate(x):\n",
        "        if i==0:\n",
        "            string = str(xi[0]) \n",
        "        else:\n",
        "            string += \" + \" + str(xi[0])\n",
        "    string1 = string + \" = \" + str(y[0])\n",
        "    string2 = string + \" = \" + str(np.sum(x))\n",
        "    print(\"Prédiction: \", string1)\n",
        "    print(\"Valeur réelle: \", string2)\n",
        "    ecart = abs(np.sum(x)-y[0])\n",
        "    print(\"Écart absolu entre X[{a}] et Y[{a}]: {b}\".format(a=idx, b=ecart))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AtN4yJLRk57F"
      },
      "source": [
        "Complétez le morceau de code suivant en déstandardisant les valeurs `xtest` et `ypred_rnn`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aDw139hpcH_",
        "outputId": "d7290b9e-495e-4f89-8142-abccd7a41061"
      },
      "source": [
        "xtest = xtest.cpu()\n",
        "ypred_rnn = ypred_rnn.cpu()\n",
        "\n",
        "# déstandardiser les données\n",
        "xtest_unstd = xtest*std + mean\n",
        "ypred_unstd = ypred_rnn*std + seq_len*mean\n",
        "\n",
        "idx = np.random.randint(len(ytest))\n",
        "\n",
        "\n",
        "print(\"\")\n",
        "print(\"Exemple RNN:\")\n",
        "print(\"\")\n",
        "\n",
        "print_sequence(xtest_unstd.detach(), ypred_unstd.detach(), idx=idx)\n",
        "\n",
        "\n",
        "\n",
        "xtest = xtest.cpu()\n",
        "ypred_lstm = ypred_lstm.cpu()\n",
        "\n",
        "xtest_unstd = xtest*std + mean\n",
        "ypred_unstd = ypred_lstm*std + seq_len*mean\n",
        "\n",
        "print(\"\")\n",
        "print(\"Exemple LSTM:\")\n",
        "print(\"\")\n",
        "\n",
        "print_sequence(xtest_unstd.detach(), ypred_unstd.detach(), idx=idx)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Exemple RNN:\n",
            "\n",
            "Prédiction:  63.0 + 10.0 + -92.0 + 45.0 + 98.0 + 99.0 + -45.0 + -53.0 + 89.0 + -33.0 + 57.0 + -36.0 + 59.999996 + 22.0 + -22.0 + -40.0 + 52.0 + 0.0 = 341.83246\n",
            "Valeur réelle:  63.0 + 10.0 + -92.0 + 45.0 + 98.0 + 99.0 + -45.0 + -53.0 + 89.0 + -33.0 + 57.0 + -36.0 + 59.999996 + 22.0 + -22.0 + -40.0 + 52.0 + 0.0 = 274.0\n",
            "Écart absolu entre X[116] et Y[116]: 67.83245849609375\n",
            "\n",
            "Exemple LSTM:\n",
            "\n",
            "Prédiction:  63.0 + 10.0 + -92.0 + 45.0 + 98.0 + 99.0 + -45.0 + -53.0 + 89.0 + -33.0 + 57.0 + -36.0 + 59.999996 + 22.0 + -22.0 + -40.0 + 52.0 + 0.0 = 255.1558\n",
            "Valeur réelle:  63.0 + 10.0 + -92.0 + 45.0 + 98.0 + 99.0 + -45.0 + -53.0 + 89.0 + -33.0 + 57.0 + -36.0 + 59.999996 + 22.0 + -22.0 + -40.0 + 52.0 + 0.0 = 274.0\n",
            "Écart absolu entre X[116] et Y[116]: 18.844192504882812\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAM5UeILQb5l"
      },
      "source": [
        "** Questions bonus**\n",
        "\n",
        "* Répétez l'exercice pour différentes opérations mathématiques (+, -, x, /, ...)\n",
        "* Faites un graphique comparant la performance des RNN et LSTM en fonction de `seq_len`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1jHLBHErzbW"
      },
      "source": [
        "---\n",
        "# Tâche 2: Modèle de langue neuronal\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h7kMH3bm1ZDz"
      },
      "source": [
        "## Objectif\n",
        "L'objectif de la deuxième partie du tutoriel est d'apprendre à générer des textes à l'aide de réseaux de neurones récurrents. En particulier, nous allons entraîner un réseau de neurones récurrents en utilisant une petite quantité de données textuelles écrites par [Shakespeare](https://en.wikipedia.org/wiki/William_Shakespeare). Une fois ce modèle entraîné, nous l'utiliserons pour générer un nouveau texte dans le style de Shakespeare.\n",
        "\n",
        "**Notez que les gens utilisent souvent le terme RNN même lorsqu'ils entraînent un LSTM. RNN est devenu le terme générique, quel que soit le type (outre les LSTM, il existe d'autres variantes couramment utilisées, telles que [les unités récurrentes à portes (URP/GRUs)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)).\n",
        "\n",
        "Le modèle que nous utiliserons est un peu plus complexe que celui de la tâche 1. Contrairement à la section précédente, tout le code est fourni (c'est-à-dire qu'il n'y a pas d'exercices à compléter). Nous vous suggérons de parcourir l'ensemble du code pour vous assurer que vous comprenez à la fois sa logique en ce qui concerne le traitement de texte mais aussi comment concevoir et entraîner un RNN pour la génération de texte. Le code pourrait être relativement facilement adapté à d'autres tâches qui pourraient vous intéresser. \n",
        "\n",
        "En outre, ce cahier contient également certaines des étapes de prétraitement des données, en particulier celles qui consistent à prendre notre ensemble de données et à l'organiser de manière à ce qu'il puisse être utilisé pour entraîner un modèle de langage neuronal basé sur les LSTM.  \n",
        "\n",
        "Bonne génération !"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-TECjbTm3L20"
      },
      "source": [
        "## Encore un peu de contexte théorique\n",
        "\n",
        "Une séquence de mots $\\mathbf{s}$ peut être représentée comme une séquence de symboles discrets $N$ (ou jetons lexicaux) telle que $\\mathbf{s} = (w_{1}, \\dots, w_{N})$, où $w_{t}$ est un mot ou un signe de ponctuation. Chaque symbole peut être représenté par un entier correspondant à son indice dans le vocabulaire $V$. $V$ contient tous les symboles d'une tâche particulière (le vocabulaire est généralement construit à partir de l'ensemble de données que nous utilisons pour une tâche). L'objectif d'un modèle de langage est d'estimer la probabilité (conjointe) d'une séquence $p(\\mathbf{s}) = p(w_{1}, \\dots, w_{N})$, qui peut être décomposée comme un produit de probabilités conditionnelles telles que :\n",
        "\n",
        "\\begin{equation}\n",
        "  p(\\mathbf{s}) = \\prod^{N}_{t=1} p(w_{t} | w_{1}, \\dots, w_{t-1}).\n",
        "\\end{equation}\n",
        "(ceci est également connu comme la règle de la chaîne dans la théorie des probabilités)\n",
        "\n",
        "C'est important pour la modélisation. En particulier, au lieu de modéliser directement la distribution conjointe, nous pouvons \"simplement\" modéliser chaque conditionnel. C'est-à-dire que nous pouvons modéliser la probabilité du mot suivant compte tenu de tous les mots précédents ($p(w_{t} | w_{<t})$). C'est précisément ce que font les modèles linguistiques, qui sont largement utilisés dans de nombreuses applications (notamment dans la traduction automatique, la reconnaissance vocale et la recherche d'informations). Notez que cela peut être compris comme un problème de classification multi-classes où les classes correspondent aux différents mots.\n",
        "\n",
        "Cependant, la modélisation de chaque condition n'est pas facile. Nous effectuons plutôt une approximation qui rend le problème plus facile. L'intuition derrière l'approximation est qu'au lieu de conditionner l'histoire entière, nous conditionnons sur une histoire plus petite ($w_{t-1}, w_{t-2}, \\ldots, w_{t-n}$) pour prédire le mot suivant ($w_{t}$). C'est ce qu'on appelle une hypothèse de Markov d'ordre $n$. Mathématiquement, c'est le cas :\n",
        "\\begin{equation}\n",
        "  p(w_{t} | w_{1}, \\dots, w_{t-1}) \\approx p(w_{t} | w_{t-n}, \\dots, w_{t-1}).\n",
        "\\end{equation}\n",
        "\n",
        "Dans le paragraphe suivant, nous expliquons comment modéliser les conditions ci-dessus en utilisant un réseau de neurones récurrent. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DYc66cIQk57O"
      },
      "source": [
        "### Modélisation à l'aide des RNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ORb7e416k57P"
      },
      "source": [
        "L'architecture du modèle de langage neuronal que nous utilisons est un réseau LSTM qui apprendra à chaque pas de temps une distribution conditionnelle du mot suivant en utilisant un certain nombre de mots précédents.\n",
        "Pour entraîner ses paramètres, nous devons d'abord fixer le nombre maximum $n$ de mots précédents à prendre en compte (`seq_len` dans le code)-- c'est la taille effective de votre historique -- pour l'entraînement $p(w_{t} | w_{t-n}, \\dots, w_{t-1})$. L'entrée dans le LSTM à chaque étape de temps est : \n",
        "- le mot $t^{th}$ $w_t$ encodé à l'aide de son *mot incorporé* (voir ci-dessous) ; \n",
        "- l'état récurrent ($\\mathbf{h}_{t-1}$) ; et \n",
        "- l'état de la mémoire au pas de temps précédent ($\\mathbf{c}_{t-1}$).\n",
        "\n",
        "La sortie du LSTM à chaque étape est le mot suivant $w_{t+1}$. C'est-à-dire que nous entraînons le LSTM à prédire le mot suivant à chaque étape (en détail, le LSTM prédira en fait la probabilité de ce mot suivant). Cela implique également qu'une fois ce modèle entraîné, nous pourrons l'utiliser pour générer du texte (nous choisirons simplement le mot ayant la plus grande probabilité et l'introduirons comme entrée à l'étape suivante). Ce type de modèle est communément connu sous le nom de *modèle de réseau de neurones basé sur LSTM*. Son architecture est présentée ci-dessous :\n",
        "\n",
        "![alt-text](https://github.com/nextai-mtl/tech-2019/blob/master/images/autoregressive_english.png?raw=true)\n",
        "\n",
        "\n",
        "Pour calculer la probabilité sur tous les mots suivants, nous utilisons simplement une fonction d'activation *softmax*. La fonction softmax renvoie un vecteur normalisé de dimension $|V|$, où chaque entrée correspond à un seul mot du vocabulaire. Chaque entrée peut être comprise comme la \"probabilité\" que le mot suivant soit le mot à cet indice.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A_0hMGr7k57Q"
      },
      "source": [
        "### Plongement de mots (mots-vecteurs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EJILkOok57Q"
      },
      "source": [
        "La principale question qui reste est de savoir comment encoder exactement les mots à chaque pas de temps. \n",
        "Comme les données d'entrée d'un réseau de neurones doivent pouvoir être encodées dans une matrice, chaque symbole (mot) $w_t$ du vocabulaire peut être représenté par un vecteur un-parmi-n *(one-hot)* $\\mathbf{x}_i$ qui est un vecteur de zéros avec un seul 1 à la position de l'index de ce mot dans le vocabulaire. Ainsi, ces vecteurs *one-hot* appartiennent à $ \\mathbb R^{|V|}$ où $|V|$ est la taille du vocabulaire, c'est-à-dire le nombre de mots dans le vocabulaire. Ces vecteurs one-hot sont multipliés par une matrice de pondération $ \\mathbf{E} \\in \\mathbb R^{|V| \\times d_{e}}$. Cette matrice est apprise et est connue sous le nom de *matrice d'incorporation*, $\\mathbf{E} \\in \\mathbb R^{|V| \\times d_{e}}$, elle encode effectivement les mots dans une représentation continue : $\\mathbf{w}_{t} \\in \\mathbb R^{d_{e}}$.\n",
        "\n",
        "En utilisant des mots-vecteurs, une séquence de mots peut donc être représentée par une séquence de vecteurs de taille $N$ $\\mathbf{s} = (\\mathbf{w}_{1}, \\dots, \\mathbf{w}_{N})$. Chaque ligne $i$ de cette matrice $\\mathbf{E}$ est une représentation en dimensions $d_{e}$ du $i$'ème mot du vocabulaire $V$. Comme nous l'avons dit plus haut, ces représentations sont souvent appelées *mots-vecteurs* ou *plongements de mots*. Lorsqu'elles sont apprises à partir d'un ensemble de données suffisamment important, elles peuvent représenter une similarité sémantique. [Pour plus d'informations sur le plongement de mots](http://ruder.io/word-embeddings-1/index.html)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1Sh2OG46_Ln"
      },
      "source": [
        "## Fonctions utilitaires de traitement de texte et structuration des données"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sf7luc-Di7qO"
      },
      "source": [
        "Pour entraîner notre modèle de langage neuronal, nous utiliserons un ensemble de données (corpus) des travaux de Shakespeare (nous avons utilisé un sous-ensemble de ce lien [https://norvig.com/ngrams/] qui a été nettoyé, tokenisé et standardisé) disponible dans le fichier `shakespeare_top20K.txt`.\n",
        "\n",
        "Pour obtenir des résultats raisonnables en pratique, nous devrions entraîner un modèle linguistique en utilisant une très grande quantité de texte. Dans ce tutoriel, nous utiliserons un corpus relativement petit de 20000 phrases, 159884 jetons, et un vocabulaire de 12354 jetons. Un token est une unité lexicale séparée par un espace de chaque côté du texte. Dans notre cas, un jeton est un mot, un nombre ou un signe de ponctuation. Le vocabulaire est l'ensemble de tous les jetons d'un corpus. (Bien entendu, notre code se généralise à des ensembles de données plus importants)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yo7yRlDyK7DA",
        "outputId": "7265270a-3de0-4b80-e578-8da8de6fa9de"
      },
      "source": [
        "# Cloner git repos pour accéder aux données\n",
        "!git clone https://github.com/nextai-mtl/tech-2019.git"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'tech-2019'...\n",
            "remote: Enumerating objects: 74, done.\u001b[K\n",
            "remote: Total 74 (delta 0), reused 0 (delta 0), pack-reused 74\u001b[K\n",
            "Unpacking objects: 100% (74/74), 999.75 KiB | 1012.00 KiB/s, done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0pjv24-kr528"
      },
      "source": [
        "START_VOCAB = [\"_UNK\"]\n",
        "UNK_ID = 0\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4SuaaJV-k57e"
      },
      "source": [
        "La fonction `create_vocabulary` prend comme entrée :\n",
        "- **corpus_path** : chemin vers les données d'un corpus\n",
        "- **vocab_path** : le chemin où un vocabulaire du corpus fourni sera créé\n",
        "- **max_vocab_size** : le nombre maximum de mots du vocabulaire\n",
        "\n",
        "Il crée un vocabulaire composé au maximum de `max_vocab_size` des jetons fréquents dans les données du corpus. Ce vocabulaire est enregistré sous le fichier `vocab_path`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sdH5sXVsDgU"
      },
      "source": [
        "def create_vocabulary(corpus_path, vocab_path, max_vocab_size=1e5):\n",
        "    \"\"\"Créer et sauvegarder le vocabulaire d'un corpus.\"\"\"\n",
        "    vocab = {}\n",
        "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        for line in f:\n",
        "            tokens = line.strip().split()\n",
        "            for token in tokens:\n",
        "                if token in vocab:\n",
        "                    vocab[token] += 1\n",
        "                else:\n",
        "                    vocab[token] = 1\n",
        "    vocab_list = START_VOCAB + sorted(vocab, key=vocab.get, reverse=True)\n",
        "    if len(vocab_list) > max_vocab_size:\n",
        "        vocab_list = vocab_list[:max_vocab_size]\n",
        "    with open(vocab_path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for token in vocab_list:\n",
        "              f.write(token + \"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTDPeBJPk57h"
      },
      "source": [
        "La fonction \" initialize_vocabulary \" prend comme entrée :\n",
        "- **vocab_path** : le chemin d'accès au fichier contenant le vocabulaire\n",
        "\n",
        "Il renvoie un tuple de deux éléments correspondant respectivement à :\n",
        "- **vocab** : un dictionnaire `Token:Index` associant un index à chaque token du vocabulaire\n",
        "- **rev_vocab** : une liste de jetons (uniques) dans le vocabulaire qui associe chaque index à un jeton tel que $vocab [rev\\_vocab[i]] = i$."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vD3GJ0wSk57i"
      },
      "source": [
        "def initialize_vocabulary(vocab_path):\n",
        "    \"\"\"Initialiser le vocabulaire.\"\"\"\n",
        "    if os.path.exists(vocab_path):\n",
        "        with open(vocab_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            rev_vocab = [line.strip() for line in f.readlines()]\n",
        "        vocab = dict([(w, i) for (i, w) in enumerate(rev_vocab)])\n",
        "        return vocab, rev_vocab\n",
        "    else:\n",
        "        raise ValueError(\"Fichier de vocabulaire {} pas trouvé.\".format(vocab_path))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OyU4gxWuk57n"
      },
      "source": [
        "La fonction `read_corpus` prend comme entrée :\n",
        "- **corpus_path** : chemin d'accès aux données d'un corpus\n",
        "\n",
        "Il renvoie les données du corpus sous la forme d'une liste de jetons."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q37P2tBFk57n"
      },
      "source": [
        "def read_corpus(corpus_path):\n",
        "    \"\"\"Lire et convertir un corpus en une liste de tokens.\"\"\"\n",
        "    with open(corpus_path, \"r\", encoding=\"utf-8\") as f:\n",
        "        corpus = f.read().split()\n",
        "    return corpus"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PsD3YDTgk57s"
      },
      "source": [
        "La fonction `corpus_to_token_ids` prend comme entrée :\n",
        "- **corpus** : le corpus comme une liste de jetons\n",
        "- **vocab** : le vocabulaire sous forme de distionnaire de la forme \"taken:index\".\n",
        "\n",
        "Il renvoie une liste de jetons-ids correspondant à la liste de jetons telle que définie par le corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0qyOa_kk57t"
      },
      "source": [
        "def corpus_to_token_ids(corpus, vocab):\n",
        "    \"\"\"Convertir un corpus en jeton-ids.\"\"\"\n",
        "    token_ids = [vocab.get(token, UNK_ID) for token in corpus]\n",
        "    return token_ids"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jYz8n0Awk57x"
      },
      "source": [
        "La fonction `batch_data` prend comme entrée :\n",
        "- **data** : liste des jeton-ids\n",
        "- **batch_size** : la taille du lot\n",
        "\n",
        "Il renvoie une version structurée des données en séquences continues de la taille d'un lot. Les données renvoyées ont une forme `N* x batch_size` où `N* = int(len(data) / batch_size)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-MrQvtMbk57y"
      },
      "source": [
        "def batch_data(data, batch_size):\n",
        "    \"\"\"Structurer les données en batch_size séquences continues.\"\"\"\n",
        "    n_batch = len(data) // batch_size\n",
        "    data = np.array(data[:n_batch*batch_size])\n",
        "    data = data.reshape(batch_size, -1).T\n",
        "    return data"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdQh_l5rk572"
      },
      "source": [
        "La fonction `detach_hidden` prend comme entrée :\n",
        "- **hidden** : état caché d'un RNN\n",
        "\n",
        "Il transforme les données de cet état caché en un nouveau tenseur avec les mêmes valeurs mais où l'historique de calcul est perdu."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kAg_tcIk573"
      },
      "source": [
        "def detach_hidden(hidden):\n",
        "    \"\"\"Transformer les données des états cachés d'un LSTM en\n",
        "       nouveaux Tensor avec require_grad=False.\"\"\"\n",
        "    return tuple(h.detach() for h in hidden)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSxEEBOEk576"
      },
      "source": [
        "Voici le morceau de code où les données du corpus sont lues, le vocabulaire est créé et les données du corpus sont structurées en séquences continues de \"batch_size\" à des fins d'entraînement."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o0mwd1xGsFM2",
        "outputId": "dd7d5571-a704-4bb3-8299-5b5f776d8b69"
      },
      "source": [
        "# Créer et initialiser le vocabulaire.\n",
        "corpus_path = \"tech-2019/data/shakespeare_top20K.txt\"\n",
        "vocab_path = \"vocab.txt\"\n",
        "\n",
        "create_vocabulary(corpus_path, vocab_path)\n",
        "vocab, rev_vocab = initialize_vocabulary(vocab_path)\n",
        "\n",
        "# Lire le corpus d'entraînement.\n",
        "corpus = read_corpus(corpus_path)\n",
        "token_ids = corpus_to_token_ids(corpus, vocab)\n",
        "\n",
        "# Structurer le corpus en batch_size séquences continues pour faire l'entraînement.\n",
        "batch_size = 10\n",
        "data = batch_data(token_ids, batch_size)\n",
        "data = torch.LongTensor(data).to(DEVICE)\n",
        "\n",
        "print(\"Nombre de tokens dans le corpus: {}\"\n",
        "      .format(len(corpus)), end=\"\\n\\n\")\n",
        "print(\"Taille du vocabulaire: {}\"\n",
        "      .format(len(vocab)), end=\"\\n\\n\")\n",
        "print(\"Liste des 20 jetons les plus fréquents dans le corpus: \\n{}\"\n",
        "      .format(rev_vocab[1:21]), end=\"\\n\\n\")\n",
        "print(\"Première phrase du corpus en format texte:\\n{}\"\n",
        "      .format(\" \".join(corpus[:31])), end=\"\\n\\n\")\n",
        "print(\"Première phrase du corpus transformée en jeton-ids:\\n{}\"\n",
        "      .format(token_ids[:31]), end=\"\\n\\n\")\n",
        "print(\"Test de conversion jeton-ids vers texte en utilisant rev_vocab:\\n{}\"\n",
        "      .format(\" \".join([rev_vocab[i] for i in token_ids[:31]])), end=\"\\n\\n\")\n",
        "print(\"Structure des données d'entraînement (observez que la première phrase \"\n",
        "      \"est dans la première colonne):\\n{}\".format(data[:20]))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Nombre de tokens dans le corpus: 159884\n",
            "\n",
            "Taille du vocabulaire: 12354\n",
            "\n",
            "Liste des 20 tokens les plus fréquents dans le corpus: \n",
            "[',', '.', 'the', 'I', ';', 'and', 'to', 'of', 'you', 'a', ':', 'in', '?', 'my', 'is', 'that', '!', 'not', 'it', 'me']\n",
            "\n",
            "Première phrase du corpus en format texte:\n",
            "A MIDSUMMER-NIGHT'S DREAM Now , fair Hippolyta , our nuptial hour Draws on apace : four happy days bring in Another moon ; but O ! methinks how slow This old\n",
            "\n",
            "Première phrase du corpus transformée en token-ids:\n",
            "[70, 5876, 5877, 194, 1, 143, 1948, 1, 58, 1425, 319, 4108, 46, 1949, 11, 607, 778, 608, 275, 12, 1950, 415, 5, 35, 62, 17, 1042, 119, 1115, 114, 173]\n",
            "\n",
            "Test de conversion token-ids vers texte en utilisant rev_vocab:\n",
            "A MIDSUMMER-NIGHT'S DREAM Now , fair Hippolyta , our nuptial hour Draws on apace : four happy days bring in Another moon ; but O ! methinks how slow This old\n",
            "\n",
            "Structure des données d'entraînement (observez que la première phrase est dans la première colonne):\n",
            "tensor([[   70,  3426,  1967,    31,    51,     1,  4276,     4,    17,   113],\n",
            "        [ 5876,     1,     1,  4954,     2,     3,     2,   123,    38,    16],\n",
            "        [ 5877,  6562,     4,   262,   303,   101,   118,    51,  2018,    28],\n",
            "        [  194,  2799,   189,    51,    90,  4639,   375,    47,  1892,    50],\n",
            "        [    1,   534,     9,    43,     9,    16,     4,  1110,     7,  1544],\n",
            "        [  143,    12,    13,  1243,    39,   161,     1,     1,  5780,   589],\n",
            "        [ 1948,    10,   128,     1,    10,  3415,   390,   327,     2,   191],\n",
            "        [    1,  1086,     1,  7779,   641,   148,     8,    51,   102, 11814],\n",
            "        [   58,     8,    28,    51,   339,  3464,    21,    64,    14,     2],\n",
            "        [ 1425,  4440,    31,    43,     2,     2,   933,  4044,   130,    53],\n",
            "        [  319,    11,   175,   113,     4,   139,     9,     5,   230,    19],\n",
            "        [ 4108,    38,    98,  3734,    55,    56,   327,     6,     1,    15],\n",
            "        [   46,  4441,    27,  1611,    34,    81,   121,    12,     4,   513],\n",
            "        [ 1949,   201,  2469,     1,     5,   159,  1623,  5018,  2868,    16],\n",
            "        [   11,     1,     6,     6,     4,    65,    10,  2632,   603,   108],\n",
            "        [  607,    12,   497,   135,    37,   642,  9637,     1,   588,    28],\n",
            "        [  778,    10,     5,   174,    54,     7,     1,  5642,     2,   350],\n",
            "        [  608,   768,  1260,    51,    19,   459,    96,    98,     4,  1467],\n",
            "        [  275,  3427,     3,   277,   180,     2,  9638,    51,    26,    27],\n",
            "        [   12,  6563,  7207,    28,    77,    53,   110,   187,   374, 11815]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n8t8iEVRk57-"
      },
      "source": [
        "Dans le bloc suivant, nous divisons les données en ensembles de données d'entraînement et de validation et nous créons les collecteurs de données correspondants. Nous utilisons 10 % des données pour l'ensemble de validation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E2EXx8HBsZAY",
        "outputId": "e3cfb38b-888c-4160-9f7d-a66a187314d4"
      },
      "source": [
        "# créer ensembles entraînement/validation et créer DataLoaders.\n",
        "X = data[:-1]\n",
        "Y = data[1:]\n",
        "\n",
        "n_valid = round(data.size(0) * 0.1)\n",
        "train_set = TensorDataset(X[:(data.size(0)-n_valid)], Y[:(data.size(0)-n_valid)])\n",
        "valid_set = TensorDataset(X[-n_valid:], Y[-n_valid:])\n",
        "\n",
        "seq_len = 40\n",
        "train_loader = DataLoader(train_set, batch_size=seq_len, shuffle=False)\n",
        "valid_loader = DataLoader(valid_set, batch_size=seq_len, shuffle=False)\n",
        "\n",
        "print(\"Ensemble d'entraînement: {} séquences de longueur {} et {} minibatches\"\n",
        "      .format(len(train_set), seq_len, len(train_loader)))\n",
        "print(\"Ensemble de validation : {} séquences de longueur {} et {} minibatches\"\n",
        "      .format(len(valid_set), seq_len, len(valid_loader)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Ensemble d'entraînement: 14389 séquences de longueur 40 et 360 minibatches\n",
            "Ensemble de validation : 1599 séquences de longueur 40 et 40 minibatches\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IAgSaVwmVxrt"
      },
      "source": [
        "## Implémentation du modèle\n",
        "\n",
        "Ci-dessous se trouve le code du *LSTM-based neural language model* décrit dans la section **Encore un peu de contexte théorique (car vous aimez vraiment ça)**.\n",
        "\n",
        "À chaque pas de temps les variables d'entrées dans le LSTM sont:\n",
        "\n",
        "1.    une minibatch de séquences de jeton-ids (c.à.d. des séquences d'indices où chaque indice représente la position d'un token dans un vocabulaire);\n",
        "2.    des tuples $(\\mathbf{h}_{0}, \\mathbf{c}_{0})$ des états récurrents et des états mémoires équivalent aux $(\\mathbf{h}_{T}, \\mathbf{c}_{T})$ de la minibatch précédente (sauf pour la première minibatch où nous initialisons les valeurs de ces variables à 0.0 avec la fonction `init_hidden()`).\n",
        "\n",
        "Chaque séquence de jeton-ids est transformée en séquence de *mots-vecteurs* en indexant les représentations *mots-vecteurs* créés par la classe [torch.nn.Embedding()](https://pytorch.org/docs/stable/nn.html?highlight=embedding#torch.nn.Embedding) qui est une matrice de paramètres de dimension $|V| \\times d_{e}$, où $|V|$ est la taille du vocabulaire (`vocab_size`) et $d_{e}$ est la dimension d'un *mot-vecteur* (`embedding_size`).\n",
        "\n",
        "Nous appliquons de l'extinction de neurones sur les *mots-vecteurs* en entrée et sur la couche de sortie pour régulariser le modèle."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0YMiwftasb4g"
      },
      "source": [
        "class LanguageModel(nn.Module):\n",
        "    \"\"\"Modèle de langue neuronal à base de LSTM.\"\"\"\n",
        "    \n",
        "    def __init__(self, vocab_size, input_size, hidden_size, n_layers=1, dropout=0.5):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "          vocab_size: taille du vocabulaire.\n",
        "          input_size: taille des mots-vecteurs.\n",
        "          hidden_size: taille des états cachés du LSTM.\n",
        "          n_layers: nombre de couches du LSTM (default: 1).\n",
        "          dropout: si non-zéro, introduit une couche d'extinction de neurones à l'entrée et à la sortie\n",
        "                   du LSTM, avec une probabilité égale d'extinction de neurones (default: 0.5).\n",
        "        \"\"\"        \n",
        "        super(LanguageModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.n_layers = n_layers\n",
        "        \n",
        "        self.embeddings = nn.Embedding(vocab_size, input_size)\n",
        "        self.lstm = nn.LSTM(input_size,\n",
        "                            hidden_size,\n",
        "                            n_layers)\n",
        "        self.linear = nn.Linear(hidden_size, vocab_size)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "        self.init_weights()\n",
        "\n",
        "    def forward(self, input, hidden):\n",
        "        embeddings = self.dropout(self.embeddings(input))\n",
        "        output, hidden = self.lstm(embeddings, hidden)\n",
        "        output = self.dropout(output)\n",
        "        result = self.linear(output.view(output.size(0)*output.size(1), output.size(2)))\n",
        "        return result.view(output.size(0), output.size(1), result.size(1)), hidden\n",
        "\n",
        "    def init_weights(self):\n",
        "        init.uniform_(self.embeddings.weight, -0.1, 0.1)\n",
        "        init.xavier_uniform_(self.linear.weight, init.calculate_gain(\"linear\"))\n",
        "        init.constant_(self.linear.bias, 0)\n",
        "        \n",
        "    def init_hidden(self, batch_size):\n",
        "        \"\"\"Initialiser les valeurs de l'état caché et la cellule du LSTM à zéro.\n",
        "        Args:\n",
        "          batch_size: taille de la mini-batch à un pas de temps.\n",
        "          \n",
        "        Returns:\n",
        "          hidden: état caché h_t et la cellule c_t à t=0 initialisés à 0, \n",
        "                  ((n_layers, batch_size, hidden_size),\n",
        "                   (n_layers, batch_size, hidden_size)).\n",
        "        \"\"\"\n",
        "        hidden = (torch.zeros(self.n_layers, batch_size, self.hidden_size, device=DEVICE),\n",
        "                  torch.zeros(self.n_layers, batch_size, self.hidden_size, device=DEVICE))\n",
        "        return hidden"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oPPkC_0GV3iu"
      },
      "source": [
        "## Entraînement du modèle\n",
        "L'entraînement du modèle se fait presque de la même manière que dans la tâche 1. Les éléments clés différents sont les suivants:\n",
        "* Nous appliquons de l'extinction de neurones sur les *mots-vecteurs* en entrée et la couche de sortie du LSTM. Étant donné que la taille de notre ensemble d'entraînement est très petite, il est alors facile pour notre modèle de faire du surapprentissage. Pour les modéles génératifs autorégressifs à base de RNN, il est fortement recommandé d'utiliser une probabilité d'extinction de neurones élevée pour éviter l'effet de surapprentissage des données.\n",
        "* Nous démarrons l'entraînement avec une valeur de `learning_rate` élevée et nous la diminuons par un facteur de 10 en fonction de la perte sur l'ensemble de validation évaluée à la fin de chaque *epoch* d'entraînement en utilisant la classe [torch.optim.lr_scheduler.ReduceLROnPlateau()](https://pytorch.org/docs/stable/optim.html?highlight=plateau#torch.optim.lr_scheduler.ReduceLROnPlateau).\n",
        "* Pour éviter le *problème de l'explosion du gradient*, nous appliquons la technique de *l'écrêtage de gradient* en normalisant la norme du gradient avec la fonction [torch.nn.utils.clip_grad_norm_()](https://pytorch.org/docs/stable/nn.html?highlight=clip#torch.nn.utils.clip_grad_norm_).\n",
        "* Nous initialisons avec des valeurs de 0.0 le tuple $(\\mathbf{h}_{0}, \\mathbf{c}_{0})$ de l'état récurrent et l'état mémoire `hidden` avec la fonction `init_hidden()` uniquement une fois au début de chaque *epoch* d'entraînement et nous propageons les nouvelles valeurs de `hidden` à travers chaque minibatch d'entraînement. En d'autres mots, les données ont été structurées à l'aide de la fonction `batch_data()` de manière à ce que nous pouvons initialiser `hidden` $(\\mathbf{h}_{0}, \\mathbf{c}_{0})$ de chaque séquence d'une nouvelle minibatch par $(\\mathbf{h}_{T}, \\mathbf{c}_{T})$ de la minibatch précédente. Un désavantage de cette méthode est que nous ne pouvons pas mélanger l'ordre des séquences à chaque *epoch* d'entraînement (c.à.d. `train_loader = DataLoader(train_set, batch_size=seq_len, shuffle=False)`).\n",
        "* Notre fonction de coût est l'entropie croisée [torch.nn.CrossEntropyLoss()](https://pytorch.org/docs/stable/nn.html?highlight=crossentropy#torch.nn.CrossEntropyLoss).\n",
        "* Nous présentons la mesure de *[Perplexité](https://en.wikipedia.org/wiki/Perplexity)* qui est une mesure d'évaluation de la qualité d'un modèle de langue."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "winuGw1rsuOe"
      },
      "source": [
        "# Construire le modèle.\n",
        "vocab_size = len(vocab)\n",
        "embedding_size = 300\n",
        "hidden_size = 400\n",
        "n_layers = 1\n",
        "dropout = 0.65\n",
        "model = LanguageModel(vocab_size, embedding_size, hidden_size, n_layers, dropout).to(DEVICE)\n",
        "print(\"Nombre de paramètres dans le modèle:\", sum(param.nelement() for param in model.parameters()))\n",
        "\n",
        "# Fonction de coût et optimiseur.\n",
        "learning_rate = 10\n",
        "loss_fun = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=2, verbose=True)\n",
        "\n",
        "# Entraînement du modèle.\n",
        "n_epochs = 20\n",
        "max_grad_norm = 1\n",
        "\n",
        "print(\"Entraînement du modèle pour {} epochs de {} minibatches\".format(n_epochs, len(train_loader)))\n",
        "for epoch in range(n_epochs):\n",
        "    model.train()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    train_loss = 0\n",
        "    valid_loss = 0\n",
        "    for x, y in train_loader:\n",
        "        # Détacher les états cachés précédents du graphe computationnel.\n",
        "        hidden = detach_hidden(hidden)\n",
        "        \n",
        "        # Réinitialiser le gradient.\n",
        "        optimizer.zero_grad()\n",
        "                \n",
        "        # propagation avant.\n",
        "        y_pred, hidden = model(x, hidden)\n",
        "        \n",
        "        # Calculer la perte.\n",
        "        loss = loss_fun(y_pred.view(-1, vocab_size), y.view(-1))\n",
        "        \n",
        "        # Calculer le gradient.\n",
        "        loss.backward()\n",
        "        \n",
        "        # Normaliser le gradient pour éviter explosion.\n",
        "        nn.utils.clip_grad_norm_(model.parameters(), max_grad_norm)\n",
        "        \n",
        "        # Mettre à jour les paramètres du modèle.\n",
        "        optimizer.step()        \n",
        "        \n",
        "        # Accumuler la perte total.\n",
        "        train_loss += len(x) * loss.item()\n",
        "    \n",
        "    # Évaluer le modèle sur l'ensemble de validation.\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        hidden = model.init_hidden(batch_size)\n",
        "        for x, y in valid_loader:\n",
        "            y_pred, hidden = model(x, hidden)\n",
        "            loss = loss_fun(y_pred.view(-1, vocab_size), y.view(-1))\n",
        "            valid_loss += len(x) * loss.item()\n",
        "    \n",
        "    train_loss /= len(train_loader.dataset)\n",
        "    valid_loss /= len(valid_loader.dataset)\n",
        "    scheduler.step(valid_loss)\n",
        "\n",
        "    print(\"Epoch {:2d} | Training loss = {:.5f} | Validation loss = {:.5f} | Perplexity = {:.2f}\"\n",
        "          .format(epoch+1, train_loss, valid_loss, np.exp(valid_loss)))\n",
        "print(\"Félicitations! Vous avez terminé d'entraîner votre beau modèle de langue neuronal!\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tKAOoUwWV8To"
      },
      "source": [
        "## Génération de texte\n",
        "\n",
        "Maintenant que nous avons entraîné un modèle linguistique, nous pouvons l'utiliser pour générer un texte comme Shakespeare ! \n",
        "Pour ce faire, nous sélectionnerons au hasard le premier mot (c'est-à-dire un jeton dans un vocabulaire) et l'utiliserons comme jeton d'entrée à la première étape. Nous utiliserons ensuite le mot de sortie au premier pas de temps comme mot d'entrée au deuxième pas de temps et ainsi de suite. Au total, nous allons générer des `n_words`.La variable `smoothing` permet d'obtenir une modification de la diversité du texte généré. Une valeur plus élevée permet de générer un texte plus diversifié mais souvent de moindre qualité."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lLg-dbOSsuVu",
        "outputId": "d480987f-d16d-410e-84e1-f68269baf7c5"
      },
      "source": [
        "# Génération de séquences de mots.\n",
        "model.eval()\n",
        "x = torch.randint(0, vocab_size, (1, 1), dtype=torch.long, device=DEVICE)\n",
        "words = [rev_vocab[x]]\n",
        "n_words = 300\n",
        "smoothing = 0.7\n",
        "with torch.no_grad(): \n",
        "    hidden = model.init_hidden(1)\n",
        "    for i in range(n_words-1):\n",
        "        output, hidden = model(x, hidden)\n",
        "        weights = output.squeeze().div(smoothing).exp()\n",
        "        word_idx = torch.multinomial(weights, 1)\n",
        "        x.fill_(word_idx.squeeze())\n",
        "        word = rev_vocab[word_idx]\n",
        "        words.append(word)\n",
        "        if (i+1) % 15 == 0:\n",
        "            words.append(\"\\n\")\n",
        "print(\" \".join(words))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "skilfully , For his 'My a hates of the The king . I are own life \n",
            " . beasts I ladyships a gentility : let are our first . I may her \n",
            " you it . I am I shall your them ; Ay you to the sings \n",
            " ; I will may them . Count shall a safer , and nothing . Be \n",
            " I am you of : I will so . And so I love ; a \n",
            " lord , and to the loving : I am with in mother . I know \n",
            " I had thy fame . I will all I do your equally ; and I \n",
            " one it's : And would be as my sun-rise not . I cave with him \n",
            " . But I say him . You was tongue is a good well . I \n",
            " am for his fortune of the ; and as you then as for such good \n",
            " of the hour ; and you shall a lord . I I am I will \n",
            " both when for a vice . I shall not with him . I will him \n",
            " ; none ; heaven me in her sworn . I am me Where . I \n",
            " am you of my way . Truly death thou doth come time . I know \n",
            " of a fairings . Will not be . You are not to I am would \n",
            " my look find ; I know a search , and you give not . Well \n",
            " of he the Holla ; I have in his Remit . You truly in this \n",
            " love . I know like she carman our pin : the lord : I may \n",
            " thorn-bush . I will know your princely . brother is them . Let not had \n",
            " dear . I have friends it a Fewness . I are traveller's ; and\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8HI-Elp2UhBY"
      },
      "source": [
        "Qu'en pensez-vous? À première vue, bien que certaines séquences de mots peuvent être cocasses, la clarté du texte généré n'est pas la meilleure. \n",
        "\n",
        "En effet, nous observons que la virgule (`,`) apparaît souvent dans le texte et cela est dû au fait que la virgule s'agit du jeton le plus fréquent dans notre corpus.  \n",
        "\n",
        "Pour améliorer la clarté du texte généré, il serait nécessaire d'entraîner le modèle sur un corpus de plus grande taille. De plus, nous soulignons que le vocabulaire et le style littéraire de notre cher Shakespeare ne sont pas les plus conventionnels! Au final, il ne faut pas oublier que nous avons simplement appris une distribution de probabilité basée sur les statistiques d'un corpus."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gUtkYczw4Pg-"
      },
      "source": [
        "---\n",
        "## Références\n",
        "* http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
        "\n",
        "* https://arxiv.org/abs/1803.08240\n",
        "\n",
        "* http://colah.github.io/posts/2015-08-Understanding-LSTMs/"
      ]
    }
  ]
}